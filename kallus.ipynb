{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MILP Formulation from Nathan Kallus' Paper (Problem 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model specifications\n",
    "#### Since we chose to modify the formulation to a certain extent, these variables simply allow us to revert back to the original model\n",
    "- delta_include: If true, then constraint 4c from original formulation holds. If false, only the added constraint that gamma[p] need to add to 1 holds\n",
    "- different_Cp = If true, then we have different sets of branching choices for every non-leaf node (like original formulation). If false, then we have a static set C for all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- This class is an alternative to solving the right/left ancestor problem --\n",
    "\"\"\"\n",
    "INPUT: d = depth of tree (which includes root node), so d = 2 would make a tree with {1, 2, 3}\n",
    "RELEVANT FUNCTIONS:\n",
    "- get_right_left: For all leaf nodes, returns its right and left ancestors in a dictionary \n",
    "                  of {(p, q): 1 or -1 if q is right or left ancestor respectively}\n",
    "\"\"\"\n",
    "class Tree:\n",
    "    def __init__(self, d):\n",
    "        self.depth = d\n",
    "        self.nodes = list(range(1, 2**(d-1)))\n",
    "        self.leaves = list(range(2**(d-1), 2**d))\n",
    "        self.ancestor_rl = {}\n",
    "    \n",
    "    def get_left_children(self, n):\n",
    "        if n in self.nodes:\n",
    "            return int(2*n)\n",
    "        else:\n",
    "            raise Exception ('Invalid node n')\n",
    "    \n",
    "    def get_right_children(self, n):\n",
    "        if n in self.nodes:\n",
    "            return int(2*n+1)\n",
    "        else:\n",
    "            raise Exception ('Invalid node n')\n",
    "    \n",
    "    def get_parent(self, n):\n",
    "        if (n in self.nodes) | (n in self.leaves):\n",
    "            return int(math.floor(n/2))\n",
    "        else:\n",
    "            raise Exception ('Invalide node n')\n",
    "    \n",
    "    def get_ancestors(self, direction, n):\n",
    "        current = n\n",
    "        ancestors = []\n",
    "        while current != 1:\n",
    "            current_buffer = self.get_parent(current)\n",
    "            if direction == 'r':\n",
    "                if self.get_right_children(current_buffer) == current:\n",
    "                    ancestors.append(current_buffer)\n",
    "            else:\n",
    "                if self.get_left_children(current_buffer) == current:\n",
    "                    ancestors.append(current_buffer)\n",
    "            current = current_buffer\n",
    "        return ancestors\n",
    "    \n",
    "    def get_right_left(self):\n",
    "        for i in self.leaves:\n",
    "            right = self.get_ancestors('r', i)\n",
    "            for j in right:\n",
    "                self.ancestor_rl[(i, j)] = 1\n",
    "            left = self.get_ancestors('l', i)\n",
    "            for j in left:\n",
    "                self.ancestor_rl[(i, j)] = -1\n",
    "        return self.ancestor_rl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    #train_X = df.iloc[:, :25]\n",
    "    train_X = df[['AGE2', 'AGE3', 'RVISINF', 'RSBP2', 'RSBP3', 'RSBP4', 'RDEF3', 'RDEF4', 'RDEF5', 'RCONSC1', 'RCONSC2']]\n",
    "    real = df[['y0', 'y1', 'y2', 'y3', 'y4', 'y5']]\n",
    "    train_t = df['t']\n",
    "    train_y = df['y']\n",
    "    return train_X, train_t, train_y, real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file_v2(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    train_X = df.iloc[:, :3]\n",
    "    real = df.iloc[:, 3:5]\n",
    "    train_t = df.iloc[:, 5]\n",
    "    train_y = df.iloc[:, 7]\n",
    "    return train_X, train_t, train_y, real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datapoint_tree(node, i, test_X, test_real, test_t):\n",
    "    if node in L: #if datapoint has reached leaf node, calculate error\n",
    "        index = treatments[node]\n",
    "        ideal_outcome = max(test_real.iloc[i, :])\n",
    "        difference = ideal_outcome - test_real.iloc[i, index]\n",
    "        if difference == 0:\n",
    "            count_optimal = 1\n",
    "        else:\n",
    "            count_optimal = 0\n",
    "        \n",
    "        if index == test_t[i]:\n",
    "            same_treatment = 1\n",
    "        else:\n",
    "            same_treatment = 0\n",
    "        return difference, count_optimal, same_treatment\n",
    "    if test_X.iloc[i, branching[node]] <= 0: # go left (node 2)\n",
    "        return datapoint_tree(tree.get_left_children(node), i, test_X, test_real, test_t)\n",
    "    else:\n",
    "        return datapoint_tree(tree.get_right_children(node), i, test_X, test_real, test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(test_X, test_real, test_t):\n",
    "    difference = 0\n",
    "    count_optimal = 0\n",
    "    count_same = 0\n",
    "    for i in range(len(test_X)):\n",
    "        diff, optimal, treat = datapoint_tree(1, i, test_X, test_real, test_t)\n",
    "        difference += diff\n",
    "        count_optimal += optimal\n",
    "        count_same += treat\n",
    "    return difference, float(count_optimal)/len(test_X), float(count_same)/len(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring variables determined a-priori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Specfiying Input to Model\n",
    "- m treatments indexed by t {1,..., m}\n",
    "- n datapoints indexed by i {(X1, T1, Y1), ..., (Xn, Tn, Yn)} \n",
    "- d: depth of decision tree\n",
    "- n_min: minimum number of datapoints of each treatment in node p\n",
    "- num_features\n",
    "- num_cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tree(depth, bert):\n",
    "    delta_include = False\n",
    "    different_Cp = False\n",
    "    bertsimas = bert\n",
    "\n",
    "    m = {0, 1, 2, 3, 4, 5}\n",
    "    n = len(train_X)\n",
    "    d = int(depth)\n",
    "    n_min = 0\n",
    "    num_features = 2\n",
    "    num_cuts = 2\n",
    "\n",
    "    \"\"\"# ---- CONSTRUCTING COMPLETE BINARY TREE ----\n",
    "    # - P = number of nodes in the tree\n",
    "    # - L_c = set of non-leaf nodes\n",
    "    # - L = set of leaf ndoes\"\"\"\n",
    "\n",
    "    P = 2**d\n",
    "    L_c = set(range(1, 2**(d-1)))\n",
    "    L = set(range(2**(d-1), P))\n",
    "\n",
    "    # - ancestors: dictionary {leaf nodes: {set of ancestors}}\n",
    "    ancestors = {}\n",
    "    for p in L:\n",
    "        ancestors[p] = [math.floor(p/(2**j)) for j in range(1, d)]\n",
    "\n",
    "        # Alternative way of retrieving right/left ancestors\n",
    "    tree = Tree(d)\n",
    "    right_left = tree.get_right_left()\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    - C[p]: finite set of cuts on node p--determined apriori {(l, theta)}\n",
    "        Representation: dictionary {non-leaf node: list of (l, theta)}\n",
    "        Require a list instead of set so it's ordered (indexed easily)\n",
    "\n",
    "\n",
    "    From Kallus' paper, he wants us to:\n",
    "    1. For each l in [d], sort data along x_l\n",
    "    2a. For each non-leaf node, pick #features from [d] randomly\n",
    "    2b. Set J = {1, (n-1/#cuts), ..., n-1} in decreasing order of cuts\n",
    "    2c. Set Cp = {(l, midpoint between the two buckets in J) for all dimensions chosen and for all j in J}\n",
    "\n",
    "    Make version of ALG3 to take all features\"\"\"\n",
    "\n",
    "\n",
    "    if different_Cp:\n",
    "        pass\n",
    "    else:\n",
    "        # -- BINARY COVARIATES --> Create a finite set of cuts for C for all features --\n",
    "        C = []\n",
    "        for i in range(len(train_X.columns)):\n",
    "            C.append((i, 0))\n",
    "\n",
    "\n",
    "    \"\"\" --- OTHER DATA --- \n",
    "    BIG M Constraints:\n",
    "    - Ybar\n",
    "    - Ymax\n",
    "    - M\n",
    "\n",
    "    BINARY ENCODING FOR CUTS\n",
    "    - k_p: dictionary {non-leaf node p: k_p value}\n",
    "    - Z_p: dictionary {non-leaf node p: k_p x |C_p| 2d matrix}\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Big M Constraints ----\n",
    "    # Ybar is merged into data, but it could not be. ybar is a numpy array\n",
    "    minimum = min(train_y)\n",
    "\n",
    "    ybar = train_y - minimum\n",
    "    #data['ybar'] = ybar\n",
    "\n",
    "    # Ymax\n",
    "    ymax = max(ybar)\n",
    "\n",
    "    # M\n",
    "    # Find all sums for treatments 1, ..., m\n",
    "    #treatment_counts = treatment.value_counts().to_list()\n",
    "    unique, counts = np.unique(train_t, return_counts=True)\n",
    "    #frequencies = numpy.asarray((unique, counts)).T\n",
    "    M = np.array(counts)\n",
    "    M -= len(L) * n_min\n",
    "    M = max(M)\n",
    "    \n",
    "    model = gp.Model(\"Kallus\")\n",
    "    #model.params.TimeLimit = 3600\n",
    "\n",
    "    # -- VARIABLE DECLARATION --\n",
    "\n",
    "    # -- Variables to determine: gamma and lambda --\n",
    "    # 1. gamma_p = choice of cut at node p ([0, 1]^C_p) (only applies to non-leaf node)\n",
    "    #       - represent with a matrix gamma (|L_c| x |C_p|)\n",
    "\n",
    "    gamma = model.addVars(L_c, len(C), vtype=GRB.BINARY, name='gamma') \n",
    "    # This assumes gamma is binary\n",
    "    #gamma = model.addVars(L_c, len(C), vtype=GRB.BINARY, name='gamma') \n",
    "\n",
    "    # 2. lambda_pt = choice of treatment t at node p (only applies to leaf nodes L)\n",
    "    #       - represent with a matrix lamb (|L| x m)\n",
    "    lamb = model.addVars(L, m, vtype=GRB.BINARY, name='lamb')\n",
    "\n",
    "\n",
    "    # -- Other Variables in Formulation --\n",
    "    # 1. w_ip = membership of datapoint i in node p (only applies to leaf nodes L)\n",
    "    #       - represent with a matrix w (n x |L|)\n",
    "\n",
    "    w = model.addVars(n, L, lb=0, ub=1, name='w')\n",
    "    # This assumes w is binary, when in reality it is continuous from 0-1\n",
    "    #w = model.addVars(n, L, vtype=GRB.BINARY, name='w') # Original paper has this be a continuous variable\n",
    "\n",
    "    # 2. mu_p = mean outcome of prescribed treatment in node p\n",
    "    #       - represent with a matrix mu (|L|)\n",
    "    mu = model.addVars(L, lb=0, name='mu') # define in constraint\n",
    "\n",
    "    # 3. nu_ip = \"effect\" of treatment in node p by multiplying mu and w\n",
    "    #       - represent with a matrix nu (n x |L|)\n",
    "    nu = model.addVars(n, L, lb=0, name='nu')\n",
    "\n",
    "    # 4. delta_p = forces only 1 choice of cut at node p\n",
    "    #       - represent with a dictionary {non-leaf node p: 1d matrix of size k_p}\n",
    "    #delta = model.addVars(L, k, vtype=GRB.BINARY, name='delta')\n",
    "\n",
    "    # 5. Chi_i(gamma) = 1 if choice of cut induces datapoint i to go left on the cut gamma, 0 otherwise\n",
    "    chi = model.addVars(L_c, n, vtype=GRB.BINARY, name='chi')\n",
    "\n",
    "    if bertsimas:\n",
    "        # 6. f_i\n",
    "        f = model.addVars(n, lb=0, name='f')\n",
    "\n",
    "        # 7. Beta_lt\n",
    "        beta = model.addVars(L, m, lb=0, name='beta')\n",
    "\n",
    "    theta = 0.5\n",
    "    \n",
    "        # --- OBJECTIVE FUNCTION ---\n",
    "    if bertsimas:\n",
    "        model.setObjective(theta * gp.quicksum(nu[i, p] for i in range(n) for p in L) \n",
    "                       - (1-theta) * gp.quicksum((train_y[i] - f[i]) * (train_y[i] - f[i]) for i in range(n)), GRB.MAXIMIZE)\n",
    "    else:\n",
    "        model.setObjective(gp.quicksum(nu[i, p] for i in range(n) for p in L), GRB.MAXIMIZE)\n",
    "\n",
    "\n",
    "    # --- CONSTRAINTS ---\n",
    "    # Constraint 4c (4b is done by definition of variables)\n",
    "    if delta_include:\n",
    "        for p in L_c:\n",
    "            # need to do matrix multiplication somehow, but this might work?\n",
    "            for j in range(k):\n",
    "                model.addConstr(delta[p, j] == gp.quicksum(gamma[p, i] * z[j, i] for i in range(len(C))))\n",
    "\n",
    "    # Additional constraint that gamma[p] adds up to 1\n",
    "    # CHECKED\n",
    "    for p in L_c:\n",
    "        model.addConstr(gp.quicksum(gamma[p, i] for i in range(len(C))) == 1)\n",
    "\n",
    "    # Add constraint Chi\n",
    "    for i in range(n):\n",
    "        for p in L_c:\n",
    "            model.addConstr(chi[p, i] == gp.quicksum(gamma[p, j] for j in range(len(C)) if C[j][1] >= train_X.iloc[i, C[j][0]]))\n",
    "\n",
    "\n",
    "    # Constraint 4d&e (Membership restriction from its ancestors) CHECKED\n",
    "    for p in L:\n",
    "        A_p = ancestors[p] #index ancestors of p\n",
    "        for q in A_p:\n",
    "            R_pq = right_left[(p, q)]\n",
    "            for i in range(n):\n",
    "                model.addConstr(w[i, p] <= (1+R_pq)/2 - R_pq * chi[q, i])\n",
    "\n",
    "\n",
    "    #4e CHECKED\n",
    "    for p in L:\n",
    "        A_p = ancestors[p] #index ancestors of p\n",
    "        for i in range(n):\n",
    "            model.addConstr(w[i, p] >= 1 + gp.quicksum(-chi[q, i] for q in A_p if right_left[(p, q)] == 1)\n",
    "                        + gp.quicksum(-1+chi[q, i] for q in A_p if right_left[(p, q)] == -1))\n",
    "\n",
    "\n",
    "    # Constraint 4f\n",
    "    # CHECKED\n",
    "    for t in m:\n",
    "        for p in L:\n",
    "            model.addConstr(gp.quicksum(w[i, p] for i in range(n) if train_t[i] == t) >= n_min) #assuming the input comes in vector (Xi, Ti, Yi)\n",
    "            # only add the datapoints that have been given treatment t\n",
    "\n",
    "    # Constraints 4g&h (Linearization of nu)\n",
    "    # CHECKED\n",
    "    for p in L:\n",
    "        for i in range(n):\n",
    "            model.addConstr(nu[i, p] <= ymax * w[i, p])\n",
    "            model.addConstr(nu[i, p] <= mu[p])\n",
    "            model.addConstr(nu[i, p] >= mu[p] - ymax * (1-w[i, p]))\n",
    "\n",
    "    # Constraint 4i (Choice of treatment applied to p)\n",
    "    # CHECKED\n",
    "    for p in L:\n",
    "        model.addConstr(gp.quicksum(lamb[p, t] for t in m) == 1)\n",
    "\n",
    "    # Constraint 4j&k (Consistency between lambda and mu)\n",
    "    # CHECKED. There are some inconsistencies where some w don't appear, but this is because ybar is 0 (i.e. the minimum)\n",
    "    for p in L:\n",
    "        for t in m:\n",
    "            model.addConstr(gp.quicksum(nu[i, p] - w[i, p] * ybar[i] for i in range(n) if train_t[i] == t) <= M*(1-lamb[p, t]))\n",
    "            model.addConstr(gp.quicksum(nu[i, p] - w[i, p] * ybar[i] for i in range(n) if train_t[i] == t) >= M*(lamb[p, t]-1))\n",
    "    #model.addConstr(lamb[2, 0] == 1)\n",
    "    if bertsimas:\n",
    "        for i in range(n):\n",
    "            for p in L:\n",
    "                for t in m:\n",
    "                    if train_t[i] == t:\n",
    "                        model.addConstr(f[i] - beta[p, t] <= M * (1-w[i, p]))\n",
    "                        model.addConstr(f[i] - beta[p, t] >= M * (w[i, p]-1))\n",
    "    \n",
    "    #model.params.TimeLimit = 3600\n",
    "    timeLimit = 3600\n",
    "    oldSolutionLimit = model.Params.SolutionLimit\n",
    "    model.Params.SolutionLimit = 1\n",
    "    model.optimize()\n",
    "    model.Params.TimeLimit = timeLimit - model.getAttr(GRB.Attr.Runtime)\n",
    "    model.Params.SolutionLimit = oldSolutionLimit - model.Params.SolutionLimit\n",
    "    model.optimize()\n",
    "    \n",
    "    \n",
    "    #model.optimize()\n",
    "    g = model.getAttr(\"X\", gamma).items()\n",
    "    l = model.getAttr(\"X\", lamb).items()\n",
    "    \n",
    "    branching = {i[0][0]: i[0][1] for i in g if i[1] == 1.0}\n",
    "    treatments = {i[0][0]: i[0][1] for i in l if i[1] == 1.0}\n",
    "    \n",
    "    return branching, treatments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'bertsimas_athey_500.csv' does not exist: b'bertsimas_athey_500.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d2b3b115bc7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mbertsimas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bertsimas_athey_500.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'bertsimas_athey_500.csv' does not exist: b'bertsimas_athey_500.csv'"
     ]
    }
   ],
   "source": [
    "\"\"\"kallus = pd.DataFrame(columns=['Dataset','Depth','P(Correct Treatment)','Test Error','Test % Optimal',\n",
    "                               'Test % Same Treatment', 'Train Error', 'Train % Optimal', \n",
    "                               'Train % Same Treatment', 'Tree'])\n",
    "bertsimas = pd.DataFrame(columns=['Dataset','Depth','P(Correct Treatment)','Test Error','Test % Optimal',\n",
    "                               'Test % Same Treatment', 'Train Error', 'Train % Optimal', \n",
    "                                  'Train % Same Treatment', 'Tree'])\"\"\"\n",
    "\n",
    "\n",
    "#bertsimas = pd.read_csv('bertsimas_athey_500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed value of parameter SolutionLimit to 1\n",
      "   Prev: 2000000000  Min: 1  Max: 2000000000  Default: 2000000000\n",
      "Gurobi Optimizer version 9.1.0 build v9.1.0rc0 (mac64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 22039 rows, 10025 columns and 74110 nonzeros\n",
      "Model fingerprint: 0x2d7d9669\n",
      "Variable types: 8002 continuous, 2023 integer (2023 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 6e+02]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 6e+02]\n",
      "Presolve removed 8008 rows and 3998 columns\n",
      "Presolve time: 0.22s\n",
      "Presolved: 14031 rows, 6027 columns, 50367 nonzeros\n",
      "Variable types: 4002 continuous, 2025 integer (2019 binary)\n",
      "\n",
      "Root relaxation: objective 2.000000e+03, 7403 iterations, 0.32 seconds\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0 2000.00000    0 1585          - 2000.00000      -     -    0s\n",
      "H    0     0                    1307.2555386 2000.00000  53.0%     -    0s\n",
      "\n",
      "Explored 0 nodes (805 simplex iterations) in 0.62 seconds\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 1: 1307.26 \n",
      "\n",
      "Solution limit reached\n",
      "Best objective 1.307255538579e+03, best bound 2.000000000000e+03, gap 52.9923%\n",
      "Changed value of parameter TimeLimit to 3599.3738889694214\n",
      "   Prev: inf  Min: 0.0  Max: inf  Default: inf\n",
      "Changed value of parameter SolutionLimit to 1999999999\n",
      "   Prev: 1  Min: 1  Max: 2000000000  Default: 2000000000\n",
      "Gurobi Optimizer version 9.1.0 build v9.1.0rc0 (mac64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 22039 rows, 10025 columns and 74110 nonzeros\n",
      "Model fingerprint: 0x2d7d9669\n",
      "Variable types: 8002 continuous, 2023 integer (2023 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 6e+02]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 6e+02]\n",
      "Presolved: 14031 rows, 6027 columns, 50367 nonzeros\n",
      "\n",
      "Continuing optimization...\n",
      "\n",
      "H    0     2                    1321.3114754 2000.00000  51.4%     -    2s\n",
      "     0     2 2000.00000    0 1146 1321.31148 2000.00000  51.4%     -    2s\n",
      "H   13    16                    1365.7967759 2000.00000  46.4%   165    4s\n",
      "    19    22 1737.32305    5    3 1365.79678 2000.00000  46.4%   401    5s\n",
      "*   29    23               5    1369.8630137 2000.00000  46.0%   344    5s\n",
      "H   32    23                    1392.1938602 2000.00000  43.7%   378    5s\n",
      "*   67    44              12    1415.2830450 2000.00000  41.3%   446    6s\n",
      "*   76    44              13    1425.2979827 2000.00000  40.3%   479    7s\n",
      "   133    46 1886.22453   10  782 1425.29798 2000.00000  40.3%   606   10s\n",
      "   207    63 1903.53532   14 1063 1425.29798 2000.00000  40.3%   669   15s\n",
      "*  285    62              12    1445.0054434 2000.00000  38.4%   616   17s\n",
      "   422    33     cutoff   10      1445.00544 1797.94911  24.4%   614   20s\n",
      "\n",
      "Explored 506 nodes (302304 simplex iterations) in 20.58 seconds\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 8: 1445.01 1425.3 1415.28 ... 1307.26\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 1.445005443371e+03, best bound 1.445005443371e+03, gap 0.0000%\n",
      "[3, 2, 0.5, 5846, 66.27631958465531, 24.464955292760312, 672, 66.4, 24.6, 'branching = {1: 2}, treatments = {2: 5, 3: 4}']\n",
      "Changed value of parameter SolutionLimit to 1\n",
      "   Prev: 2000000000  Min: 1  Max: 2000000000  Default: 2000000000\n",
      "Gurobi Optimizer version 9.1.0 build v9.1.0rc0 (mac64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 54079 rows, 22061 columns and 185258 nonzeros\n",
      "Model fingerprint: 0x17f7c4c3\n",
      "Variable types: 16004 continuous, 6057 integer (6057 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 6e+02]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 6e+02]\n",
      "Presolve removed 30 rows and 12 columns\n",
      "Presolve time: 0.47s\n",
      "Presolved: 54049 rows, 22049 columns, 177240 nonzeros\n",
      "Variable types: 16004 continuous, 6045 integer (6045 binary)\n",
      "Found heuristic solution: objective 1217.0212766\n",
      "\n",
      "Explored 0 nodes (0 simplex iterations) in 0.70 seconds\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 1: 1217.02 \n",
      "\n",
      "Solution limit reached\n",
      "Best objective 1.217021276596e+03, best bound 8.000000000000e+03, gap 557.3427%\n",
      "Changed value of parameter TimeLimit to 3599.292333126068\n",
      "   Prev: inf  Min: 0.0  Max: inf  Default: inf\n",
      "Changed value of parameter SolutionLimit to 1999999999\n",
      "   Prev: 1  Min: 1  Max: 2000000000  Default: 2000000000\n",
      "Gurobi Optimizer version 9.1.0 build v9.1.0rc0 (mac64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 54079 rows, 22061 columns and 185258 nonzeros\n",
      "Model fingerprint: 0x17f7c4c3\n",
      "Variable types: 16004 continuous, 6057 integer (6057 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 6e+02]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 6e+02]\n",
      "Presolved: 54049 rows, 22049 columns, 177240 nonzeros\n",
      "\n",
      "Continuing optimization...\n",
      "\n",
      "\n",
      "Root relaxation: objective 3.979000e+03, 27601 iterations, 1.15 seconds\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "H    0     2                    1383.9147727 8000.00000   478%     - 1574s\n",
      "H    0     2                    1390.5659355 8000.00000   475%     - 1574s\n",
      "     0     2 3979.00000    0 5946 1390.56594 3979.00000   186%     - 1574s\n",
      "     1     4 2844.00000    1 2546 1390.56594 3935.75000   183% 34577 1599s\n",
      "H    3     8                    1392.1938602 3128.50000   125% 14622 1606s\n",
      "     7    12 2000.00000    3 1709 1392.19386 2990.00000   115%  7832 1610s\n",
      "H   31    40                    1445.0054434 2990.00000   107%  1977 1614s\n",
      "    39    46 1905.97015   10    4 1445.00544 2990.00000   107%  1621 1615s\n",
      "    51    50     cutoff   13      1445.00544 2809.00000  94.4%  1450 1620s\n",
      "    96    89 1626.53846    9  454 1445.00544 2809.00000  94.4%  1067 1626s\n",
      "   174   133     cutoff   22      1445.00544 2694.00000  86.4%   738 1630s\n",
      "   246   154 1713.17481   10 1264 1445.00544 2693.50000  86.4%   685 1635s\n",
      "   355   207 1930.23297   11 1382 1445.00544 2545.50000  76.2%   556 1640s\n",
      "*  395   241              15    1445.0483805 2545.50000  76.2%   547 1643s\n",
      "H  397   241                    1445.2133274 2545.50000  76.1%   546 1643s\n",
      "   458   262 2000.00000    7  781 1445.21333 2000.00000  38.4%   526 1649s\n",
      "   510   303 2000.00000    8    8 1445.21333 2000.00000  38.4%   518 1660s\n",
      "H  564   298                    1456.6634266 2000.00000  37.3%   497 1660s\n",
      "   568   327 1923.18962   16    5 1456.66343 2000.00000  37.3%   500 1674s\n",
      "   645   345 1670.29890   13 1871 1456.66343 2000.00000  37.3%   499 1683s\n",
      "H  660   345                    1456.6634873 2000.00000  37.3%   493 1683s\n",
      "   722   418 2000.00000   11    9 1456.66349 2000.00000  37.3%   492 1688s\n",
      "   841   439     cutoff   22      1456.66349 2000.00000  37.3%   469 1697s\n",
      "   895   538 1901.02898   12    5 1456.66349 2000.00000  37.3%   469 1703s\n",
      "  1115   620 1660.93750   13    4 1456.66349 2000.00000  37.3%   454 1708s\n",
      "  1328   738 1850.34646   16    5 1456.66349 2000.00000  37.3%   437 1714s\n",
      "  1565   839 1857.68310   10 1233 1456.66349 2000.00000  37.3%   421 1719s\n",
      "  1819   875 1910.80172   11 1202 1456.66349 2000.00000  37.3%   407 1736s\n",
      "H 1862   842                    1472.9976559 2000.00000  35.8%   406 1736s\n",
      "  1908   962     cutoff   17      1472.99766 2000.00000  35.8%   404 1742s\n",
      "H 1971   955                    1475.3579044 2000.00000  35.6%   405 1742s\n",
      "  2201   956 2000.00000   12  269 1475.35790 2000.00000  35.6%   394 1830s\n",
      "  2203   957 1623.43109   19  471 1475.35790 2000.00000  35.6%   394 1843s\n",
      "  2205   959 1643.97842   19  471 1475.35790 2000.00000  35.6%   393 1845s\n",
      "  2206   962 2000.00000   16  330 1475.35790 2000.00000  35.6%   399 1895s\n",
      "  2208   966 2000.00000   17   90 1475.35790 2000.00000  35.6%   401 1930s\n",
      "H 2322   979                    1475.3579078 2000.00000  35.6%   393 1934s\n",
      "  2329   985 1549.97956   33  164 1475.35791 2000.00000  35.6%   392 1935s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H 2457   977                    1477.6002551 2000.00000  35.4%   381 1938s\n",
      "  2502  1043 2000.00000   21   97 1477.60026 2000.00000  35.4%   376 1940s\n",
      "  3048  1036     cutoff   42      1477.60026 2000.00000  35.4%   332 1947s\n",
      "  3090  1040 1554.42526   39  115 1477.60026 2000.00000  35.4%   329 1950s\n",
      "  3602  1133 1493.33730   31  127 1477.60026 2000.00000  35.4%   300 1955s\n",
      "  3883  1100 infeasible   34      1477.60026 2000.00000  35.4%   289 1960s\n",
      "  4706  1194 1477.94157   36  162 1477.60026 2000.00000  35.4%   268 1965s\n",
      "  5648  1562 2000.00000   23   89 1477.60026 2000.00000  35.4%   248 1970s\n",
      "  6253  1732 2000.00000   27    5 1477.60026 2000.00000  35.4%   238 1976s\n",
      "  6696  1940 1845.12918   38    5 1477.60026 2000.00000  35.4%   233 1980s\n",
      "  6999  2073     cutoff   36      1477.60026 2000.00000  35.4%   229 1985s\n",
      "  7954  2499 2000.00000   29    7 1477.60026 2000.00000  35.4%   217 1990s\n",
      "  8532  2731 1493.89565   36    2 1477.60026 2000.00000  35.4%   214 1995s\n",
      "  9571  3150 1502.98858   39   80 1477.60026 1995.65306  35.1%   206 2001s\n",
      " 10109  3270 1955.99231   35  304 1477.60026 1994.02299  35.0%   202 2005s\n",
      "*10334  3257              37    1479.2182947 1994.02299  34.8%   202 2005s\n",
      " 10452  3519 1665.59894   41    2 1479.21829 1994.02299  34.8%   202 2010s\n",
      " 11657  3808     cutoff   35      1479.21829 1986.24561  34.3%   197 2015s\n",
      " 12728  4063 1925.49923   35  128 1479.21829 1982.95161  34.1%   193 2020s\n",
      " 13183  4059     cutoff   32      1479.21829 1981.00000  33.9%   192 2026s\n",
      " 13627  4304 1523.15802   29    2 1479.21829 1980.00000  33.9%   191 2030s\n",
      " 14909  4514 1713.02595   35  194 1479.21829 1975.37705  33.5%   188 2036s\n",
      "*15265  4468              31    1491.6182947 1974.42039  32.4%   187 2038s\n",
      " 15554  4587 1954.29394   29  161 1491.61829 1973.00000  32.3%   187 2040s\n",
      " 16595  4959     cutoff   35      1491.61829 1970.04608  32.1%   186 2046s\n",
      " 17239  5108 1510.41856   39    2 1491.61829 1967.21951  31.9%   185 2051s\n",
      " 17881  5339     cutoff   39      1491.61829 1963.53386  31.6%   184 2056s\n",
      " 18688  5591     cutoff   38      1491.61829 1962.11429  31.5%   184 2060s\n",
      " 19289  5659 1578.11243   37  199 1491.61829 1959.83273  31.4%   184 2085s\n",
      " 20199  5925 1582.60636   32  147 1491.61829 1956.51163  31.2%   183 2091s\n",
      " 21102  6010 1553.91987   33  269 1491.61829 1954.37967  31.0%   181 2249s\n",
      " 21104  6011 1896.72521   31  471 1491.61829 1954.37967  31.0%   181 2251s\n",
      " 21106  6013 1805.94240   28  470 1491.61829 1954.37967  31.0%   181 2255s\n",
      " 21108  6014 1875.41997   27  513 1491.61829 1954.37967  31.0%   181 2260s\n",
      " 21115  6019 1580.72618   32  340 1491.61829 1954.37967  31.0%   181 2268s\n",
      " 21117  6020 1949.57639   25    6 1491.61829 1954.37967  31.0%   181 2276s\n",
      "H21117  5718                    1520.8098392 1954.37967  28.5%   181 2282s\n",
      " 21119  5719 1937.00000   33   90 1520.80984 1954.37967  28.5%   181 2285s\n",
      " 21120  5720 1732.54023   36    8 1520.80984 1954.37967  28.5%   181 2294s\n",
      " 21121  5721 1874.07826   32    8 1520.80984 1954.37967  28.5%   181 2299s\n",
      " 21122  5721 1520.80984   34    8 1520.80984 1954.37967  28.5%   181 2301s\n",
      " 21123  5725 1954.37967   31    8 1520.80984 1954.37967  28.5%   183 2341s\n",
      " 21125  5728 1954.37967   32    8 1520.80984 1954.37967  28.5%   183 2366s\n",
      " 21129  5731 1954.37967   33  379 1520.80984 1954.37967  28.5%   183 2391s\n",
      " 21137  5736 1954.37967   34  206 1520.80984 1954.37967  28.5%   183 2406s\n",
      " 21141  5739 1954.37967   34  149 1520.80984 1954.37967  28.5%   183 2418s\n",
      " 21145  5743 1954.37967   35  207 1520.80984 1954.37967  28.5%   183 2447s\n",
      " 21150  5747 1954.37967   35  499 1520.80984 1954.37967  28.5%   183 2470s\n",
      " 21156  5752 1954.37967   36  326 1520.80984 1954.37967  28.5%   183 2475s\n",
      " 21175  5780 1954.37967   38    7 1520.80984 1954.37967  28.5%   183 2486s\n",
      " 21273  5836 infeasible   48      1520.80984 1954.37967  28.5%   184 2493s\n",
      " 21347  5933 1833.56250   49   87 1520.80984 1954.37967  28.5%   185 2497s\n",
      " 21567  5934     cutoff   55      1520.80984 1954.37967  28.5%   186 2500s\n",
      " 22050  5940     cutoff   46      1520.80984 1954.37967  28.5%   188 2506s\n",
      " 22295  5943 1954.37967   49  303 1520.80984 1954.37967  28.5%   189 2510s\n",
      " 22458  5948     cutoff   56      1520.80984 1954.37967  28.5%   190 2516s\n",
      " 22550  5946 1954.37967   51  323 1520.80984 1954.37967  28.5%   191 2520s\n",
      " 22729  5940     cutoff   48      1520.80984 1954.37967  28.5%   192 2526s\n",
      " 22887  5891 1861.77143   49  287 1520.80984 1954.37967  28.5%   194 2534s\n",
      " 22909  5941 1785.79031   51  125 1520.80984 1954.37967  28.5%   194 2536s\n",
      " 23041  5930 1761.90082   60    5 1520.80984 1954.37967  28.5%   194 2540s\n",
      " 23272  5912 1734.63416   45  146 1520.80984 1954.37967  28.5%   196 2546s\n",
      " 23603  5945 1954.37967   45   64 1520.80984 1954.37967  28.5%   198 2552s\n",
      " 23786  5931 1954.37967   50  311 1520.80984 1954.37967  28.5%   199 2556s\n",
      " 24133  5927 1954.37967   40  416 1520.80984 1954.37967  28.5%   201 2565s\n",
      " 24496  5965 1751.95946   44  125 1520.80984 1954.37967  28.5%   202 2573s\n",
      " 24695  5983 1954.37967   43   90 1520.80984 1954.37967  28.5%   203 2576s\n",
      " 24894  5998 1954.37967   41  426 1520.80984 1954.37967  28.5%   204 2580s\n",
      " 25329  6069 1852.81223   41  317 1520.80984 1954.37967  28.5%   206 2588s\n",
      " 25605  6062 1839.06542   42   98 1520.80984 1954.37967  28.5%   207 2593s\n",
      " 25842  5990 1655.83626   41  203 1520.80984 1954.37967  28.5%   208 2601s\n",
      " 25898  6086 1856.98387   40  290 1520.80984 1954.37967  28.5%   208 2605s\n",
      " 26185  6073 1709.37182   39  300 1520.80984 1954.37967  28.5%   209 2610s\n",
      " 26412  5994 1587.20924   43  120 1520.80984 1954.37967  28.5%   211 2634s\n",
      " 26423  6080     cutoff   39      1520.80984 1954.37967  28.5%   211 2638s\n",
      " 26749  6066 1711.53749   44  123 1520.80984 1954.37967  28.5%   211 2644s\n",
      " 27103  6040 1850.70533   40  409 1520.80984 1954.37967  28.5%   212 2648s\n",
      " 27431  6030 1954.37967   44  162 1520.80984 1954.37967  28.5%   213 2653s\n",
      " 27713  6073 1625.72388   54  143 1520.80984 1954.37967  28.5%   214 2658s\n",
      " 28104  6113 1954.37967   41  291 1520.80984 1954.37967  28.5%   214 2664s\n",
      " 28534  6104 1874.47820   49  204 1520.80984 1954.37967  28.5%   215 2669s\n",
      " 29028  6041 1954.37967   46  148 1520.80984 1954.37967  28.5%   215 2676s\n",
      " 29451  6022 1658.58181   48  213 1520.80984 1954.37967  28.5%   216 2684s\n",
      " 29924  5897     cutoff   53      1520.80984 1954.37967  28.5%   217 2692s\n",
      " 30029  6054 1641.94676   51  224 1520.80984 1954.37967  28.5%   218 2699s\n",
      " 30646  6000 1665.07785   44  181 1520.80984 1954.37967  28.5%   218 2706s\n",
      " 31082  6031     cutoff   55      1520.80984 1954.37967  28.5%   218 2714s\n",
      " 31457  5906     cutoff   52      1520.80984 1954.37967  28.5%   218 2715s\n",
      " 31729  6063 1541.66304   50  125 1520.80984 1954.37967  28.5%   219 2727s\n",
      " 32569  5840     cutoff   56      1520.80984 1954.37967  28.5%   220 2733s\n",
      " 32813  5888 1827.16667   51  427 1520.80984 1954.37967  28.5%   221 2739s\n",
      " 33182  5892 1583.83680   50  147 1520.80984 1954.37967  28.5%   222 2746s\n",
      " 33605  5892 1792.71711   45  290 1520.80984 1954.37967  28.5%   223 2753s\n",
      " 34100  5730 1565.27200   50   83 1520.80984 1954.37967  28.5%   223 2763s\n",
      " 34127  5844 1821.27742   41  308 1520.80984 1954.37967  28.5%   223 2769s\n",
      " 34600  5868     cutoff   51      1520.80984 1954.37967  28.5%   224 2775s\n",
      " 35114  5910 1742.82929   52  238 1520.80984 1954.37967  28.5%   224 2781s\n",
      " 35702  5737 1665.82042   52   86 1520.80984 1954.37967  28.5%   224 2788s\n",
      " 35838  5805 1717.54232   49  235 1520.80984 1954.37967  28.5%   225 2794s\n",
      " 36265  5771 1549.33289   46  169 1520.80984 1954.37967  28.5%   225 2799s\n",
      " 36630  5735 1629.13168   54    2 1520.80984 1954.37967  28.5%   226 2804s\n",
      " 37056  5680 1954.37967   42  134 1520.80984 1954.37967  28.5%   227 2810s\n",
      " 37483  5676 1649.11231   51  229 1520.80984 1954.37967  28.5%   228 2816s\n",
      " 37894  5622     cutoff   56      1520.80984 1954.37967  28.5%   228 2822s\n",
      " 38266  5633 1865.09037   47  238 1520.80984 1954.37967  28.5%   229 2827s\n",
      " 38788  5735     cutoff   57      1520.80984 1954.37967  28.5%   229 2833s\n",
      " 39237  5824 1624.60074   58   89 1520.80984 1954.37967  28.5%   230 2840s\n",
      " 39576  5997 1659.88541   55   84 1520.80984 1954.37967  28.5%   230 2846s\n",
      " 40124  6127 1748.87389   48  128 1520.80984 1954.37967  28.5%   231 2852s\n",
      " 40568  6231 1713.49737   42  247 1520.80984 1954.37967  28.5%   231 2858s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 40903  6286 1953.50995   40  165 1520.80984 1954.37967  28.5%   232 2864s\n",
      " 41224  6386 1799.49784   46  270 1520.80984 1954.37967  28.5%   232 2869s\n",
      " 41588  6484 1578.23367   46  193 1520.80984 1954.37967  28.5%   233 2875s\n",
      " 42028  6655 1850.80311   43  303 1520.80984 1954.37967  28.5%   233 2885s\n",
      " 42767  6726 1550.96576   48  145 1520.80984 1954.37967  28.5%   234 2891s\n",
      " 43092  6856 1810.85714   49  285 1520.80984 1954.37967  28.5%   235 2897s\n",
      " 43481  6983 1873.69247   43  374 1520.80984 1954.37967  28.5%   235 2904s\n",
      " 43916  7102     cutoff   60      1520.80984 1954.37967  28.5%   236 2910s\n",
      " 44319  7179 1827.90575   47  367 1520.80984 1954.37967  28.5%   236 2916s\n",
      " 44721  7293 1700.23391   51  151 1520.80984 1954.37967  28.5%   237 2922s\n",
      " 45130  7406 1842.96688   45   82 1520.80984 1954.37967  28.5%   237 2928s\n",
      " 45603  7500 1954.37967   46  157 1520.80984 1954.37967  28.5%   238 2934s\n",
      " 45923  7624 1644.18726   52  293 1520.80984 1954.37967  28.5%   238 2940s\n",
      " 46383  7714 1841.92012   45   84 1520.80984 1954.37967  28.5%   238 2947s\n",
      " 46889  7835     cutoff   45      1520.80984 1954.37967  28.5%   238 2952s\n",
      " 47368  7956 1691.13600   52  237 1520.80984 1954.37967  28.5%   239 2958s\n",
      " 47801  8035 1764.20436   43  302 1520.80984 1954.37967  28.5%   239 2965s\n",
      " 48224  8157 1758.25796   46  321 1520.80984 1954.37967  28.5%   239 2971s\n",
      " 48715  8246 1766.28621   47  288 1520.80984 1954.37967  28.5%   239 2977s\n",
      " 49142  8266 1789.75546   45  124 1520.80984 1954.37967  28.5%   240 2999s\n",
      " 49190  8309     cutoff   46      1520.80984 1954.37967  28.5%   240 3005s\n",
      " 49622  8399 1831.04279   50   70 1520.80984 1954.37967  28.5%   240 3011s\n",
      " 50115  8482 1602.61325   54  112 1520.80984 1954.37967  28.5%   241 3016s\n",
      " 50523  8620     cutoff   44      1520.80984 1954.37967  28.5%   241 3023s\n",
      " 51001  8705 1954.37967   48  291 1520.80984 1954.37967  28.5%   242 3028s\n",
      " 51370  8778 1947.53850   49  371 1520.80984 1954.37967  28.5%   243 3034s\n",
      " 51772  8924 1616.39352   53  157 1520.80984 1954.37967  28.5%   243 3040s\n",
      " 52327  9142 1621.47813   46  145 1520.80984 1954.37967  28.5%   243 3051s\n",
      " 53064  9222 1567.65572   45  145 1520.80984 1954.37967  28.5%   244 3056s\n",
      " 53406  9278 1697.96390   42  144 1520.80984 1954.37967  28.5%   244 3062s\n",
      " 53760  9339     cutoff   49      1520.80984 1954.37967  28.5%   244 3068s\n",
      " 54190  9394     cutoff   45      1520.80984 1954.37967  28.5%   245 3073s\n",
      " 54577  9514 1707.91958   52  168 1520.80984 1954.37967  28.5%   245 3080s\n",
      " 55110  9601 1751.03873   50   92 1520.80984 1954.37967  28.5%   245 3085s\n",
      " 55588  9684     cutoff   53      1520.80984 1954.37967  28.5%   245 3091s\n",
      " 56007  9784 1689.02196   47  144 1520.80984 1954.37967  28.5%   246 3096s\n",
      " 56370  9867 1779.89492   50  164 1520.80984 1954.37967  28.5%   246 3102s\n",
      " 56836  9949     cutoff   55      1520.80984 1954.37967  28.5%   246 3107s\n",
      " 57322 10051     cutoff   51      1520.80984 1954.37967  28.5%   246 3112s\n",
      " 57724 10106 1829.04630   45  315 1520.80984 1954.37967  28.5%   246 3118s\n",
      " 58147 10218 1954.37967   44  130 1520.80984 1954.37967  28.5%   246 3123s\n",
      " 58530 10309 1672.26092   47  129 1520.80984 1954.37967  28.5%   247 3129s\n",
      " 58904 10384 1849.08108   49   85 1520.80984 1954.37967  28.5%   247 3134s\n",
      " 59204 10444 1689.57668   48  152 1520.80984 1954.37967  28.5%   247 3140s\n",
      " 59528 10500 1811.70468   51  147 1520.80984 1954.37967  28.5%   247 3145s\n",
      " 59916 10546     cutoff   48      1520.80984 1954.37967  28.5%   248 3151s\n",
      " 60324 10636     cutoff   53      1520.80984 1954.37967  28.5%   248 3156s\n",
      " 60745 10748 1902.13809   50  161 1520.80984 1954.37967  28.5%   248 3162s\n",
      " 61229 10865 1823.04823   48  147 1520.80984 1954.37967  28.5%   248 3167s\n",
      " 61725 10934 1701.87781   48   83 1520.80984 1954.37967  28.5%   248 3172s\n",
      " 62053 10999     cutoff   51      1520.80984 1954.37967  28.5%   248 3178s\n",
      " 62411 11102     cutoff   49      1520.80984 1954.37967  28.5%   249 3183s\n",
      " 62835 11239 1698.52559   50  158 1520.80984 1954.37967  28.5%   249 3192s\n",
      " 63467 11317 1576.69469   55   84 1520.80984 1954.37967  28.5%   249 3197s\n",
      " 63868 11376 1588.81890   50  124 1520.80984 1954.37967  28.5%   249 3201s\n",
      " 64174 11487 1606.92216   52  207 1520.80984 1954.37967  28.5%   250 3207s\n",
      " 64669 11575     cutoff   51      1520.80984 1954.37967  28.5%   250 3212s\n",
      " 65033 11635 1664.46970   49  128 1520.80984 1954.37967  28.5%   250 3217s\n",
      " 65442 11662 1676.73347   41  182 1520.80984 1954.37967  28.5%   250 3223s\n",
      " 65742 11748 1713.69136   53  243 1520.80984 1954.37967  28.5%   250 3228s\n",
      " 66208 11818 1719.36175   51   84 1520.80984 1954.37967  28.5%   250 3233s\n",
      " 66593 12001 1530.79365   49  208 1520.80984 1954.37967  28.5%   250 3239s\n",
      " 67121 12061 1786.12805   44  132 1520.80984 1954.37967  28.5%   250 3244s\n",
      " 67470 12121     cutoff   53      1520.80984 1954.37967  28.5%   251 3249s\n",
      " 67844 12178 1765.96097   50  224 1520.80984 1954.37967  28.5%   251 3254s\n",
      " 68338 12260     cutoff   53      1520.80984 1954.37967  28.5%   251 3260s\n",
      " 68700 12327 1893.51417   51  231 1520.80984 1954.37967  28.5%   251 3270s\n",
      " 68871 12441 1754.27443   53   88 1520.80984 1954.37967  28.5%   251 3275s\n",
      " 69370 12516     cutoff   47      1520.80984 1954.37967  28.5%   252 3281s\n",
      " 69899 12594     cutoff   43      1520.80984 1954.37967  28.5%   252 3286s\n",
      " 70260 12697 1954.37967   38    6 1520.80984 1954.37967  28.5%   252 3291s\n",
      " 70737 12800 1843.15464   47  203 1520.80984 1954.37967  28.5%   252 3297s\n",
      " 71134 12917 1619.07749   46  130 1520.80984 1954.37967  28.5%   252 3302s\n",
      " 71595 12980 1839.95301   45  235 1520.80984 1954.37967  28.5%   252 3308s\n",
      " 72071 13065     cutoff   45      1520.80984 1954.37967  28.5%   253 3313s\n",
      " 72420 13139 1756.44060   51    3 1520.80984 1954.37967  28.5%   253 3319s\n",
      " 72871 13205 1572.57842   58   83 1520.80984 1954.37967  28.5%   253 3324s\n",
      " 73252 13289 1928.68161   45  314 1520.80984 1954.37967  28.5%   254 3332s\n",
      " 73830 13291     cutoff   52      1520.80984 1954.37967  28.5%   254 3364s\n",
      " 73836 13378     cutoff   51      1520.80984 1954.37967  28.5%   254 3370s\n",
      " 74287 13488 1645.60152   50  145 1520.80984 1954.37967  28.5%   254 3375s\n",
      " 74874 13549 1563.30028   51   92 1520.80984 1954.37967  28.5%   254 3381s\n",
      " 75258 13712 1884.04565   45  250 1520.80984 1954.37967  28.5%   254 3386s\n",
      " 75911 13759 1938.49364   48  212 1520.80984 1954.37967  28.5%   254 3390s\n",
      " 76191 13830 1562.87732   53   82 1520.80984 1954.37967  28.5%   254 3395s\n",
      " 76851 13960     cutoff   58      1520.80984 1954.37967  28.5%   254 3404s\n",
      " 77213 14055 1557.10333   49    2 1520.80984 1954.37967  28.5%   255 3409s\n",
      " 77721 14117 1623.62966   55  290 1520.80984 1954.37967  28.5%   254 3413s\n",
      " 78064 14187 1660.95999   46  214 1520.80984 1954.19048  28.5%   255 3418s\n",
      " 78436 14239     cutoff   46      1520.80984 1953.49341  28.5%   255 3422s\n",
      " 78764 14316 1548.39113   49  167 1520.80984 1953.27862  28.4%   255 3427s\n",
      " 79124 14418 1903.97316   48  269 1520.80984 1952.38849  28.4%   255 3432s\n",
      " 79568 14510 1543.01693   53  123 1520.80984 1951.85450  28.3%   255 3436s\n",
      " 79948 14591 1839.80426   53  168 1520.80984 1951.29730  28.3%   255 3442s\n",
      " 80329 14671 1814.04656   55   73 1520.80984 1950.86792  28.3%   255 3448s\n",
      " 80736 14728 1927.11811   45  256 1520.80984 1950.45471  28.3%   255 3453s\n",
      " 81089 14785     cutoff   51      1520.80984 1950.18750  28.2%   255 3458s\n",
      " 81482 14877 1827.77784   43  209 1520.80984 1949.53761  28.2%   255 3463s\n",
      " 81928 15024 1655.63549   57   86 1520.80984 1948.87705  28.1%   255 3468s\n",
      " 82437 15095 1822.83333   42   71 1520.80984 1948.32326  28.1%   254 3472s\n",
      " 82815 15164 1698.24238   49  149 1520.80984 1947.65902  28.1%   254 3477s\n",
      " 83192 15220 1681.96922   50  128 1520.80984 1947.15528  28.0%   255 3481s\n",
      " 83521 15296     cutoff   52      1520.80984 1946.71978  28.0%   255 3489s\n",
      " 84092 15371 1580.14571   46  163 1520.80984 1946.00000  28.0%   255 3494s\n",
      " 84583 15459     cutoff   51      1520.80984 1945.44811  27.9%   255 3499s\n",
      " 84980 15498     cutoff   51      1520.80984 1944.72289  27.9%   255 3503s\n",
      " 85325 15543 1808.01963   47  126 1520.80984 1944.33363  27.8%   255 3508s\n",
      " 85794 15613 1780.48323   48  309 1520.80984 1943.53739  27.8%   255 3513s\n",
      " 86208 15677 1914.11467   40  304 1520.80984 1943.00000  27.8%   255 3518s\n",
      " 86556 15724 1805.89976   46  302 1520.80984 1942.66667  27.7%   255 3523s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 86956 15800 1733.47203   54  274 1520.80984 1942.33333  27.7%   255 3528s\n",
      " 87350 15900 1784.21265   51   88 1520.80984 1941.64354  27.7%   256 3533s\n",
      " 87781 15978 1804.72376   43  132 1520.80984 1940.89135  27.6%   256 3538s\n",
      " 88143 16065 1779.28934   42  287 1520.80984 1940.09677  27.6%   256 3542s\n",
      " 88567 16101     cutoff   53      1520.80984 1939.39672  27.5%   256 3547s\n",
      " 88942 16164 1829.06111   51  203 1520.80984 1938.88793  27.5%   256 3552s\n",
      " 89388 16216     cutoff   56      1520.80984 1938.06686  27.4%   256 3556s\n",
      " 89792 16260 1726.51347   52  177 1520.80984 1937.58365  27.4%   256 3561s\n",
      " 90181 16317 1879.76744   46  210 1520.80984 1936.94231  27.4%   256 3566s\n",
      " 90620 16393 1722.94790   49  175 1520.80984 1936.20094  27.3%   256 3572s\n",
      " 91102 16444 1771.30808   50  126 1520.80984 1935.44707  27.3%   256 3576s\n",
      " 91513 16523     cutoff   50      1520.80984 1934.82243  27.2%   256 3581s\n",
      " 91925 16600     cutoff   55      1520.80984 1934.38819  27.2%   256 3586s\n",
      " 92391 16655     cutoff   59      1520.80984 1933.67352  27.1%   256 3590s\n",
      " 92751 16723 1898.52603   51  272 1520.80984 1932.90909  27.1%   256 3596s\n",
      " 93239 16843 1764.71608   59   88 1520.80984 1931.94853  27.0%   256 3600s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 87\n",
      "  Implied bound: 4\n",
      "  MIR: 2617\n",
      "  Flow cover: 19\n",
      "  RLT: 2341\n",
      "  Relax-and-lift: 22\n",
      "  BQP: 253\n",
      "\n",
      "Explored 93673 nodes (23978616 simplex iterations) in 3599.39 seconds\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 10: 1520.81 1491.62 1479.22 ... 1445.21\n",
      "\n",
      "Time limit reached\n",
      "Best objective 1.520809839154e+03, best bound 1.931392857143e+03, gap 26.9977%\n",
      "[3, 3, 0.5, 5396, 68.87222382463224, 19.417363715027403, 598, 70.1, 18.6, 'branching = {1: 6, 2: 2, 3: 7}, treatments = {4: 1, 5: 4, 6: 5, 7: 4}']\n",
      "Changed value of parameter SolutionLimit to 1\n",
      "   Prev: 2000000000  Min: 1  Max: 2000000000  Default: 2000000000\n",
      "Gurobi Optimizer version 9.1.0 build v9.1.0rc0 (mac64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 22039 rows, 10025 columns and 74032 nonzeros\n",
      "Model fingerprint: 0x7ebc7111\n",
      "Variable types: 8002 continuous, 2023 integer (2023 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 6e+02]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 6e+02]\n",
      "Presolve removed 8010 rows and 4000 columns\n",
      "Presolve time: 0.23s\n",
      "Presolved: 14029 rows, 6025 columns, 50310 nonzeros\n",
      "Variable types: 4002 continuous, 2023 integer (2017 binary)\n",
      "\n",
      "Root relaxation: objective 2.000000e+03, 7108 iterations, 0.28 seconds\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0 2000.00000    0    6          - 2000.00000      -     -    0s\n",
      "H    0     0                    1296.2231462 2000.00000  54.3%     -    0s\n",
      "\n",
      "Explored 1 nodes (805 simplex iterations) in 0.74 seconds\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 1: 1296.22 \n",
      "\n",
      "Solution limit reached\n",
      "Best objective 1.296223146223e+03, best bound 2.000000000000e+03, gap 54.2944%\n",
      "Changed value of parameter TimeLimit to 3599.2544100284576\n",
      "   Prev: inf  Min: 0.0  Max: inf  Default: inf\n",
      "Changed value of parameter SolutionLimit to 1999999999\n",
      "   Prev: 1  Min: 1  Max: 2000000000  Default: 2000000000\n",
      "Gurobi Optimizer version 9.1.0 build v9.1.0rc0 (mac64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 22039 rows, 10025 columns and 74032 nonzeros\n",
      "Model fingerprint: 0x7ebc7111\n",
      "Variable types: 8002 continuous, 2023 integer (2023 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 6e+02]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 6e+02]\n",
      "Presolved: 14029 rows, 6025 columns, 50310 nonzeros\n",
      "\n",
      "Continuing optimization...\n",
      "\n",
      "     2     4 2000.00000    1 1569 1296.22315 2000.00000  54.3%   2.5   19s\n",
      "H    4     8                    1360.3603604 2000.00000  47.0%  25.5   27s\n",
      "H    7     8                    1373.4000000 2000.00000  45.6%   608   27s\n",
      "    31    23 1439.08861    6    2 1373.40000 2000.00000  45.6%   683   30s\n",
      "H   66    34                    1398.1829574 2000.00000  43.0%   556   31s\n",
      "   140    51 1481.20690   12    2 1398.18296 1985.68973  42.0%   605   36s\n",
      "   243    54 1588.75079   14  581 1398.18296 1967.55752  40.7%   609   40s\n",
      "   409    48     cutoff   14      1398.18296 1894.94423  35.5%   622   45s\n",
      "*  428    48              15    1400.6519142 1893.69054  35.2%   621   45s\n",
      "\n",
      "Explored 559 nodes (333378 simplex iterations) in 46.54 seconds\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 5: 1400.65 1398.18 1373.4 ... 1296.22\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 1.400651914200e+03, best bound 1.400651914200e+03, gap 0.0000%\n",
      "[4, 2, 0.5, 5072, 70.74127487741563, 15.540813383328524, 597, 70.15, 15.049999999999999, 'branching = {1: 9}, treatments = {2: 4, 3: 5}']\n",
      "Changed value of parameter SolutionLimit to 1\n",
      "   Prev: 2000000000  Min: 1  Max: 2000000000  Default: 2000000000\n",
      "Gurobi Optimizer version 9.1.0 build v9.1.0rc0 (mac64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 54079 rows, 22061 columns and 185056 nonzeros\n",
      "Model fingerprint: 0x13d3a76b\n",
      "Variable types: 16004 continuous, 6057 integer (6057 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 6e+02]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 6e+02]\n",
      "Presolve removed 36 rows and 18 columns\n",
      "Presolve time: 0.49s\n",
      "Presolved: 54043 rows, 22043 columns, 177027 nonzeros\n",
      "Variable types: 16004 continuous, 6039 integer (6039 binary)\n",
      "Found heuristic solution: objective 1238.0952381\n",
      "\n",
      "Explored 0 nodes (0 simplex iterations) in 0.78 seconds\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 1: 1238.1 \n",
      "\n",
      "Solution limit reached\n",
      "Best objective 1.238095238095e+03, best bound 8.000000000000e+03, gap 546.1538%\n",
      "Changed value of parameter TimeLimit to 3599.219158887863\n",
      "   Prev: inf  Min: 0.0  Max: inf  Default: inf\n",
      "Changed value of parameter SolutionLimit to 1999999999\n",
      "   Prev: 1  Min: 1  Max: 2000000000  Default: 2000000000\n",
      "Gurobi Optimizer version 9.1.0 build v9.1.0rc0 (mac64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 54079 rows, 22061 columns and 185056 nonzeros\n",
      "Model fingerprint: 0x13d3a76b\n",
      "Variable types: 16004 continuous, 6057 integer (6057 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 6e+02]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 6e+02]\n",
      "Presolved: 54043 rows, 22043 columns, 177027 nonzeros\n",
      "\n",
      "Continuing optimization...\n",
      "\n",
      "\n",
      "Root relaxation: objective 3.970000e+03, 28644 iterations, 1.37 seconds\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "H    0     2                    1250.8581612 8000.00000   540%     -  860s\n",
      "H    0     2                    1278.3956942 8000.00000   526%     -  860s\n",
      "     0     2 3970.00000    0 5913 1278.39569 3970.00000   211%     -  860s\n",
      "     1     4 2470.55370    1 2950 1278.39569 3917.00000   206% 36051  898s\n",
      "     3     8 2000.00000    2 1256 1278.39569 3392.00000   165% 15731  909s\n",
      "H    4     8                    1321.4639254 3392.00000   157% 11798  909s\n",
      "H    6     8                    1398.1829574 3131.50000   124%  9038  909s\n",
      "     7    12 2000.00000    3 1148 1398.18296 3131.50000   124%  7968  913s\n",
      "    11    16 2000.00000    3    7 1398.18296 3131.50000   124%  5279  918s\n",
      "    19    24 2000.00000    5    6 1398.18296 3131.50000   124%  3199  920s\n",
      "    52    57     cutoff   11      1398.18296 2992.80000   114%  1522  929s\n",
      "H   63    63                    1403.3733333 2992.80000   113%  1356  931s\n",
      "   111   114 1934.00000   10    4 1403.37333 2992.80000   113%   901  936s\n",
      "H  178   172                    1410.0924132 2992.80000   112%   701  938s\n",
      "   224   189 1670.76299   19  914 1410.09241 2992.80000   112%   630  942s\n",
      "   271   198 1536.75380   21 1657 1410.09241 2992.80000   112%   621  945s\n",
      "H  353   212                    1411.2502822 2992.80000   112%   573  949s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   354   210     cutoff   24      1411.25028 2992.80000   112%   571  951s\n",
      "H  357   210                    1421.9964167 2992.80000   110%   568  951s\n",
      "   399   239 2000.00000    5  871 1421.99642 2740.00000  92.7%   544  955s\n",
      "H  408   239                    1422.0396497 2740.00000  92.7%   545  955s\n",
      "H  441   267                    1422.1867926 2740.00000  92.7%   521  957s\n",
      "   451   287 1917.30331   14    5 1422.18679 2740.00000  92.7%   518  960s\n",
      "H  468   286                    1427.1867926 2740.00000  92.0%   509  960s\n",
      "H  503   280                    1441.0412370 2740.00000  90.1%   489  960s\n",
      "   562   315     cutoff   21      1441.04124 2000.00000  38.8%   483  965s\n",
      "   654   393 1916.78002   18  825 1441.04124 2000.00000  38.8%   468  970s\n",
      "   734   426 1617.15013   11 1607 1441.04124 2000.00000  38.8%   460  978s\n",
      "H  739   396                    1461.3545883 2000.00000  36.9%   459  978s\n",
      "   800   421 1601.75173   12 1690 1461.35459 2000.00000  36.9%   445  990s\n",
      "   863   463 1974.78652   10 2476 1461.35459 2000.00000  36.9%   435  999s\n",
      "H  915   463                    1461.3546132 2000.00000  36.9%   418  999s\n",
      "   936   473 1944.14286    9 3020 1461.35461 2000.00000  36.9%   418 1050s\n",
      "   964   504 1654.93478   16    3 1461.35461 2000.00000  36.9%   416 1055s\n",
      "  1042   505 1970.00000   10  267 1461.35461 2000.00000  36.9%   426 1122s\n",
      "  1044   506 1570.76786   16  453 1461.35461 2000.00000  36.9%   425 1131s\n",
      "  1047   511 2000.00000   12  298 1461.35461 2000.00000  36.9%   436 1175s\n",
      "  1049   515 2000.00000   13  331 1461.35461 2000.00000  36.9%   440 1190s\n",
      "  1276   606 1523.27347   35  143 1461.35461 2000.00000  36.9%   395 1195s\n",
      "  1863   678 1503.07113   30  144 1461.35461 2000.00000  36.9%   320 1201s\n",
      "  2044   695 1613.76513   31    2 1461.35461 2000.00000  36.9%   305 1206s\n",
      "  2775   834     cutoff   39      1461.35461 2000.00000  36.9%   274 1210s\n",
      "  3380  1046 1973.60730   23  166 1461.35461 2000.00000  36.9%   249 1215s\n",
      "  4041  1355     cutoff   26      1461.35461 2000.00000  36.9%   233 1220s\n",
      "  4740  1725 1928.44898   26   84 1461.35461 2000.00000  36.9%   228 1225s\n",
      "  5667  2035 1653.99958   25  165 1461.35461 2000.00000  36.9%   216 1230s\n",
      "  6252  2260     cutoff   31      1461.35461 2000.00000  36.9%   210 1239s\n",
      "  6284  2348 1773.66890   30    4 1461.35461 2000.00000  36.9%   210 1240s\n",
      "* 7029  2481              34    1490.3126810 2000.00000  34.2%   203 1243s\n",
      "  7360  2667 1641.51191   20   98 1490.31268 2000.00000  34.2%   200 1246s\n",
      "  8275  3006 1814.95177   27  178 1490.31268 1989.65993  33.5%   196 1250s\n",
      "  9009  3152     cutoff   33      1490.31268 1986.10154  33.3%   191 1255s\n",
      " 10036  3578 1932.61764   18  121 1490.31268 1980.47692  32.9%   187 1260s\n",
      " 11072  3963     cutoff   39      1490.31268 1976.40336  32.6%   184 1267s\n",
      " 12030  4246 1910.73790   27  118 1490.31268 1972.00000  32.3%   181 1271s\n",
      " 12906  4387 1656.20430   28  130 1490.31268 1968.21176  32.1%   179 1278s\n",
      " 13005  4550 1640.59643   29  132 1490.31268 1968.21176  32.1%   179 1281s\n",
      " 13849  4783     cutoff   38      1490.31268 1964.67606  31.8%   178 1285s\n",
      " 14658  5015 1729.55959   25  122 1490.31268 1960.47059  31.5%   177 1290s\n",
      " 15520  5209 1521.14074   32  142 1490.31268 1957.88618  31.4%   174 1295s\n",
      " 16308  5456 1620.61905   26   80 1490.31268 1953.78222  31.1%   172 1300s\n",
      " 17855  5720 1865.71726   19  152 1490.31268 1948.59091  30.8%   170 1306s\n",
      " 18437  5965 1843.77305   30    4 1490.31268 1944.54275  30.5%   169 1311s\n",
      " 19344  6144     cutoff   32      1490.31268 1941.52542  30.3%   170 1315s\n",
      " 20239  6270 1772.02722   29    4 1490.31268 1938.44828  30.1%   170 1324s\n",
      "H20243  6125                    1500.4722770 1938.44828  29.2%   170 1324s\n",
      " 20245  6205     cutoff   29      1500.47228 1938.41776  29.2%   170 1326s\n",
      " 20714  6206 1638.48556   32  267 1500.47228 1936.59091  29.1%   171 1515s\n",
      " 20719  6209 1717.99054   22  311 1500.47228 1936.59091  29.1%   171 1521s\n",
      " 20727  6215 1666.83514   31  265 1500.47228 1936.59091  29.1%   171 1527s\n",
      " 20729  6216 1798.40682   29  265 1500.47228 1936.59091  29.1%   171 1531s\n",
      " 20730  6217 1911.51579   23    6 1500.47228 1936.59091  29.1%   171 1539s\n",
      " 20731  6217 1687.52308   27  208 1500.47228 1936.59091  29.1%   171 1544s\n",
      " 20732  6218 1619.29268   23    6 1500.47228 1936.59091  29.1%   171 1549s\n",
      " 20733  6219 1905.70909   26    6 1500.47228 1936.59091  29.1%   171 1552s\n",
      " 20735  6223 1936.59091   23  212 1500.47228 1936.59091  29.1%   173 1610s\n",
      " 20737  6226 1936.59091   24    5 1500.47228 1936.59091  29.1%   173 1660s\n",
      " 20741  6229 1936.59091   25    4 1500.47228 1936.59091  29.1%   173 1697s\n",
      " 20745  6232 1936.59091   25  317 1500.47228 1936.59091  29.1%   173 1714s\n",
      " 20749  6234 1936.59091   26    8 1500.47228 1936.59091  29.1%   173 1755s\n",
      " 20757  6242 1936.59091   27  360 1500.47228 1936.59091  29.1%   173 1783s\n",
      " 20763  6247 1936.59091   27    6 1500.47228 1936.59091  29.1%   173 1786s\n",
      " 20790  6301 1936.59091   31  294 1500.47228 1936.59091  29.1%   173 1794s\n",
      " 20837  6365 1936.59091   35  101 1500.47228 1936.59091  29.1%   173 1798s\n",
      " 20981  6383 1936.59091   33  158 1500.47228 1936.59091  29.1%   175 1801s\n",
      " 21210  6427 1936.59091   32    5 1500.47228 1936.59091  29.1%   176 1805s\n",
      "H21236  6095                    1508.8380436 1936.59091  28.3%   176 1805s\n",
      " 21499  6146 1515.53333   37    4 1508.83804 1936.59091  28.3%   176 1810s\n",
      " 21831  6143 1694.51653   39   80 1508.83804 1936.59091  28.3%   178 1818s\n",
      " 21851  6221 1536.94204   40    2 1508.83804 1936.59091  28.3%   178 1820s\n",
      " 22301  6235 1900.26190   38  163 1508.83804 1936.59091  28.3%   179 1826s\n",
      " 22555  6248 1536.84967   39  121 1508.83804 1936.59091  28.3%   181 1834s\n",
      " 22729  6242 1936.59091   35  357 1508.83804 1936.59091  28.3%   181 1836s\n",
      " 23005  6253 1936.59091   36  365 1508.83804 1936.59091  28.3%   183 1843s\n",
      " 23119  6246 1936.59091   37  353 1508.83804 1936.59091  28.3%   184 1846s\n",
      " 23174  6280 1571.00946   42  101 1508.83804 1936.59091  28.3%   184 1850s\n",
      " 23348  6256 1936.59091   31  346 1508.83804 1936.59091  28.3%   185 1859s\n",
      " 23440  6295 1746.90672   38  147 1508.83804 1936.59091  28.3%   185 1862s\n",
      " 23662  6289     cutoff   34      1508.83804 1936.59091  28.3%   186 1866s\n",
      " 23886  6301 1511.62593   50  144 1508.83804 1936.59091  28.3%   187 1870s\n",
      " 24452  6233 1884.90515   33    8 1508.83804 1936.59091  28.3%   189 1877s\n",
      " 24709  6245 1713.06098   38   82 1508.83804 1936.59091  28.3%   189 1881s\n",
      " 25044  6146 1532.67050   36  156 1508.83804 1936.59091  28.3%   190 1888s\n",
      " 25112  6220 1936.59091   31  394 1508.83804 1936.59091  28.3%   190 1892s\n",
      " 25380  6194 1788.32170   36    4 1508.83804 1936.59091  28.3%   191 1896s\n",
      " 25908  6255 1936.59091   30  325 1508.83804 1936.59091  28.3%   193 1903s\n",
      " 26187  6241 1837.29706   36  247 1508.83804 1936.59091  28.3%   193 1907s\n",
      " 26420  6285 1851.02850   35  320 1508.83804 1936.59091  28.3%   194 1912s\n",
      " 26750  6270 1692.51314   43  130 1508.83804 1936.59091  28.3%   195 1917s\n",
      " 27139  6227 1936.59091   38  112 1508.83804 1936.59091  28.3%   195 1921s\n",
      " 27431  6205 1773.16946   35  205 1508.83804 1936.59091  28.3%   196 1926s\n",
      " 27774  6226 1936.59091   36  273 1508.83804 1936.59091  28.3%   197 1930s\n",
      " 28333  6112 1701.73146   38  144 1508.83804 1936.59091  28.3%   199 1958s\n",
      " 28338  6279 1668.62908   39  144 1508.83804 1936.59091  28.3%   199 1962s\n",
      " 28795  6206 1601.35146   45  155 1508.83804 1936.59091  28.3%   199 1967s\n",
      " 29171  6116 1712.13563   36  148 1508.83804 1936.59091  28.3%   200 1972s\n",
      " 29338  6136 1633.61275   37   86 1508.83804 1936.59091  28.3%   200 1978s\n",
      " 29706  6129 1545.56997   46  143 1508.83804 1936.59091  28.3%   201 1984s\n",
      " 30087  6112 1580.91274   39    4 1508.83804 1936.59091  28.3%   203 1990s\n",
      " 30498  6063 1662.42391   39  341 1508.83804 1936.59091  28.3%   204 1995s\n",
      " 30906  5990 1815.41980   36  302 1508.83804 1936.59091  28.3%   205 2009s\n",
      " 31148  6124 1812.06835   37  232 1508.83804 1936.59091  28.3%   205 2021s\n",
      " 32020  5888 1606.67229   47  144 1508.83804 1936.59091  28.3%   207 2027s\n",
      " 32303  5865 1728.94444   39   90 1508.83804 1936.59091  28.3%   208 2034s\n",
      " 32656  5881 1772.01768   41  226 1508.83804 1936.59091  28.3%   209 2041s\n",
      " 33057  5907 infeasible   41      1508.83804 1936.59091  28.3%   210 2047s\n",
      " 33594  5864     cutoff   44      1508.83804 1936.59091  28.3%   210 2053s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34054  5716     cutoff   47      1508.83804 1936.59091  28.3%   211 2063s\n",
      " 34074  5852 1933.51111   34    7 1508.83804 1936.59091  28.3%   211 2068s\n",
      " 34541  5769     cutoff   43      1508.83804 1936.59091  28.3%   212 2074s\n",
      " 34827  5753 1536.33333   47    2 1508.83804 1936.59091  28.3%   213 2079s\n",
      " 35198  5647     cutoff   36      1508.83804 1936.59091  28.3%   214 2087s\n",
      " 35270  5742 1936.59091   33  147 1508.83804 1936.59091  28.3%   214 2092s\n",
      " 35700  5698 1555.28421   42   82 1508.83804 1936.59091  28.3%   215 2098s\n",
      " 36155  5614 1579.57189   45  144 1508.83804 1936.59091  28.3%   215 2104s\n",
      " 36552  5558     cutoff   45      1508.83804 1936.59091  28.3%   216 2110s\n",
      " 36955  5535 1641.21639   40  127 1508.83804 1936.59091  28.3%   217 2115s\n",
      " 37370  5506     cutoff   41      1508.83804 1936.59091  28.3%   217 2121s\n",
      " 37783  5373     cutoff   40      1508.83804 1936.59091  28.3%   218 2130s\n",
      " 37887  5461 1573.32742   43  142 1508.83804 1936.59091  28.3%   218 2136s\n",
      " 38377  5384 1843.84962   40    4 1508.83804 1936.59091  28.3%   219 2143s\n",
      " 38778  5440 1701.09859   40  126 1508.83804 1936.59091  28.3%   220 2148s\n",
      " 39133  5505 1788.06169   39  148 1508.83804 1936.59091  28.3%   221 2154s\n",
      " 39492  5613 1560.83442   44  124 1508.83804 1936.59091  28.3%   221 2159s\n",
      " 39923  5696 1833.96226   40   82 1508.83804 1936.59091  28.3%   222 2165s\n",
      " 40393  5799 1788.60648   38  130 1508.83804 1936.59091  28.3%   223 2172s\n",
      " 40834  5897 1666.13839   39  154 1508.83804 1936.59091  28.3%   223 2178s\n",
      " 41182  6018 1530.78059   37  121 1508.83804 1936.59091  28.3%   224 2185s\n",
      " 41771  6137 1582.30879   47  144 1508.83804 1936.59091  28.3%   225 2194s\n",
      " 42288  6274 1921.31429   42    7 1508.83804 1936.59091  28.3%   225 2200s\n",
      " 42788  6358     cutoff   45      1508.83804 1936.59091  28.3%   226 2207s\n",
      " 43251  6453 1588.11137   39  142 1508.83804 1936.59091  28.3%   227 2213s\n",
      " 43679  6536     cutoff   43      1508.83804 1936.59091  28.3%   227 2219s\n",
      " 44097  6627 1780.53890   43  158 1508.83804 1936.59091  28.3%   228 2225s\n",
      " 44507  6702     cutoff   43      1508.83804 1936.59091  28.3%   228 2231s\n",
      " 44924  6745 1634.18915   48  108 1508.83804 1936.59091  28.3%   228 2236s\n",
      " 45316  6762 1617.44143   40  144 1508.83804 1936.59091  28.3%   229 2242s\n",
      " 45701  6823 1709.81638   38  194 1508.83804 1936.59091  28.3%   230 2248s\n",
      " 46152  6877 1748.35685   36  102 1508.83804 1936.59091  28.3%   230 2254s\n",
      " 46576  6939 1595.36463   40   80 1508.83804 1936.59091  28.3%   230 2259s\n",
      " 46926  7027 1576.16201   36    2 1508.83804 1936.59091  28.3%   231 2265s\n",
      " 47328  7134     cutoff   41      1508.83804 1936.59091  28.3%   231 2271s\n",
      " 47848  7203     cutoff   40      1508.83804 1936.59091  28.3%   231 2278s\n",
      " 48394  7256 1832.54174   42   99 1508.83804 1936.53333  28.3%   232 2283s\n",
      " 48893  7318     cutoff   37      1508.83804 1935.57895  28.3%   232 2289s\n",
      " 49418  7403 1615.01463   39  118 1508.83804 1934.10638  28.2%   232 2294s\n",
      " 49916  7478 1690.27233   36  220 1508.83804 1933.04870  28.1%   232 2300s\n",
      " 50346  7545 1519.65571   41  124 1508.83804 1931.11538  28.0%   232 2305s\n",
      " 50754  7594 1769.02713   36  120 1508.83804 1928.76829  27.8%   233 2310s\n",
      " 51155  7687     cutoff   45      1508.83804 1927.80473  27.8%   233 2316s\n",
      " 51597  7777     cutoff   40      1508.83804 1926.14085  27.7%   233 2321s\n",
      " 52180  7917 1604.29167   47   58 1508.83804 1923.50000  27.5%   233 2330s\n",
      " 53239  8020 1762.59973   37   64 1508.83804 1919.47368  27.2%   234 2338s\n",
      " 53583  8061 1643.42484   40  122 1508.83804 1918.08824  27.1%   234 2342s\n",
      " 54005  8142 1603.15414   38  176 1508.83804 1915.73418  27.0%   234 2347s\n",
      " 54366  8199     cutoff   43      1508.83804 1913.60589  26.8%   234 2351s\n",
      " 54683  8243 1644.82138   43  122 1508.83804 1912.85714  26.8%   234 2356s\n",
      " 55045  8292 1561.71637   50  144 1508.83804 1911.71937  26.7%   234 2361s\n",
      " 55361  8331 1649.26579   45  155 1508.83804 1909.50000  26.6%   235 2365s\n",
      " 55652  8378 1821.47536   44    5 1508.83804 1908.23316  26.5%   235 2371s\n",
      " 56073  8445 1514.90265   38    2 1508.83804 1904.89436  26.2%   235 2375s\n",
      " 56429  8508 1731.53495   39  126 1508.83804 1903.06403  26.1%   235 2380s\n",
      " 56902  8540 1740.55363   43    4 1508.83804 1901.75714  26.0%   235 2385s\n",
      " 57270  8583 1684.13082   34   98 1508.83804 1899.56792  25.9%   235 2390s\n",
      " 57643  8636     cutoff   43      1508.83804 1898.19672  25.8%   235 2395s\n",
      " 58077  8693 1542.18325   42  155 1508.83804 1895.53392  25.6%   236 2400s\n",
      " 58478  8758 1721.31418   36  248 1508.83804 1893.26923  25.5%   236 2405s\n",
      " 58868  8835 1557.00173   44  119 1508.83804 1889.95011  25.3%   236 2411s\n",
      " 59284  8900     cutoff   48      1508.83804 1887.12948  25.1%   236 2415s\n",
      " 59709  8946 1755.67895   44   78 1508.83804 1884.48734  24.9%   236 2421s\n",
      " 60026  8995     cutoff   39      1508.83804 1883.40000  24.8%   236 2426s\n",
      " 60453  9024     cutoff   41      1508.83804 1881.51897  24.7%   236 2431s\n",
      " 60895  9062     cutoff   42      1508.83804 1879.23305  24.5%   237 2436s\n",
      " 61287  9116 1706.02310   33  137 1508.83804 1877.05898  24.4%   237 2441s\n",
      " 61716  9156 1598.45823   45  142 1508.83804 1876.00000  24.3%   237 2447s\n",
      " 62188  9205 1778.07657   38   81 1508.83804 1872.63830  24.1%   237 2452s\n",
      " 62684  9288     cutoff   40      1508.83804 1870.83142  24.0%   237 2459s\n",
      " 63286  9327 1699.01047   41  125 1508.83804 1868.00000  23.8%   237 2465s\n",
      " 63729  9360 1616.03545   42    2 1508.83804 1866.95652  23.7%   238 2470s\n",
      " 64130  9411 1835.23887   36    5 1508.83804 1865.80702  23.7%   238 2475s\n",
      " 64547  9487     cutoff   44      1508.83804 1864.21622  23.6%   238 2480s\n",
      " 65348  9582 1602.55294   43    4 1508.83804 1861.41892  23.4%   238 2489s\n",
      " 65750  9621 1653.84478   37  279 1508.83804 1860.17857  23.3%   238 2494s\n",
      " 66169  9669     cutoff   41      1508.83804 1857.62751  23.1%   238 2498s\n",
      " 66526  9695 1671.12411   39  125 1508.83804 1856.73611  23.1%   238 2503s\n",
      " 66874  9724 1555.88464   39  113 1508.83804 1855.57326  23.0%   238 2508s\n",
      " 67293  9748 1819.23077   41    5 1508.83804 1853.88191  22.9%   238 2513s\n",
      " 67673  9782     cutoff   40      1508.83804 1852.87400  22.8%   239 2519s\n",
      " 68052  9780 1613.06443   42    2 1508.83804 1850.15189  22.6%   239 2524s\n",
      " 68380  9850     cutoff   40      1508.83804 1848.88549  22.5%   239 2528s\n",
      " 68803  9881     cutoff   46      1508.83804 1847.51579  22.4%   239 2532s\n",
      " 69196  9893 1829.96368   41   99 1508.83804 1845.54375  22.3%   239 2536s\n",
      " 69525  9883 1584.55387   44  142 1508.83804 1844.71605  22.3%   239 2540s\n",
      " 70159  9896 1742.98289   43    3 1508.83804 1842.57008  22.1%   239 2548s\n",
      " 70557  9933 1805.56067   44   99 1508.83804 1841.33634  22.0%   240 2552s\n",
      " 70975  9946 1692.37409   39  160 1508.83804 1840.54902  22.0%   240 2557s\n",
      " 71411  9973 1709.57625   42  108 1508.83804 1840.35288  22.0%   240 2561s\n",
      " 71774  9976 1547.79313   48  144 1508.83804 1838.84676  21.9%   240 2565s\n",
      " 72575 10058     cutoff   37      1508.83804 1836.31837  21.7%   240 2574s\n",
      " 73062 10101 1601.09901   44  144 1508.83804 1835.85906  21.7%   240 2579s\n",
      " 73561 10136 1682.90162   38    3 1508.83804 1834.38009  21.6%   240 2583s\n",
      " 73978 10158 1545.68430   43  155 1508.83804 1833.51923  21.5%   240 2587s\n",
      " 74383 10239 1683.87195   37  156 1508.83804 1832.53901  21.5%   240 2591s\n",
      " 74852 10268 1612.31005   43    2 1508.83804 1831.64720  21.4%   240 2596s\n",
      " 75311 10282 1596.67377   42    4 1508.83804 1830.93724  21.3%   240 2600s\n",
      " 76092 10331 1534.42048   35  143 1508.83804 1828.74336  21.2%   241 2608s\n",
      " 76573 10372 1529.08515   46  142 1508.83804 1827.93219  21.1%   241 2612s\n",
      " 76973 10402 1539.05893   43   78 1508.83804 1827.05281  21.1%   241 2616s\n",
      " 77445 10410     cutoff   42      1508.83804 1826.26353  21.0%   241 2621s\n",
      " 77722 10429 1722.67652   39   81 1508.83804 1825.76562  21.0%   241 2625s\n",
      " 78498 10448 1544.48586   41   78 1508.83804 1824.29474  20.9%   241 2633s\n",
      " 78863 10455 1551.36726   47  142 1508.83804 1823.30828  20.8%   241 2637s\n",
      " 79267 10480 1647.62255   38  148 1508.83804 1822.15857  20.8%   241 2642s\n",
      " 79736 10502     cutoff   39      1508.83804 1821.05095  20.7%   241 2646s\n",
      " 80448 10504 1566.58489   41  155 1508.83804 1819.06667  20.6%   242 2653s\n",
      " 80715 10496     cutoff   41      1508.83804 1818.76471  20.5%   242 2657s\n",
      " 81122 10480     cutoff   47      1508.83804 1817.89864  20.5%   242 2661s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 81549 10474     cutoff   43      1508.83804 1817.15103  20.4%   242 2665s\n",
      " 82313 10463     cutoff   48      1508.83804 1815.14478  20.3%   242 2673s\n",
      " 82671 10458 1658.78793   36  153 1508.83804 1814.06014  20.2%   242 2677s\n",
      " 83018 10470 1695.75533   43  127 1508.83804 1813.08629  20.2%   243 2682s\n",
      " 83478 10481     cutoff   43      1508.83804 1812.13017  20.1%   243 2687s\n",
      " 83947 10493 1602.12174   38    4 1508.83804 1811.19595  20.0%   243 2691s\n",
      " 84353 10504 1625.00193   38  120 1508.83804 1810.26797  20.0%   243 2695s\n",
      " 84824 10505     cutoff   39      1508.83804 1809.44917  19.9%   243 2700s\n",
      " 85543 10512 1538.17595   43  159 1508.83804 1807.44928  19.8%   243 2709s\n",
      " 85930 10517 1555.16707   37   85 1508.83804 1806.81034  19.7%   243 2713s\n",
      " 86351 10504 1783.57691   45  167 1508.83804 1806.39167  19.7%   243 2717s\n",
      " 86685 10542 1622.94264   38  144 1508.83804 1805.61184  19.7%   243 2721s\n",
      " 87111 10548 1524.10849   41  143 1508.83804 1804.45861  19.6%   243 2725s\n",
      " 87441 10541     cutoff   46      1508.83804 1803.60497  19.5%   243 2730s\n",
      " 88289 10557     cutoff   38      1508.83804 1801.56815  19.4%   243 2738s\n",
      " 88704 10549     cutoff   47      1508.83804 1800.48936  19.3%   244 2743s\n",
      " 89134 10532 1526.11884   40  142 1508.83804 1798.99392  19.2%   244 2747s\n",
      " 89504 10520     cutoff   42      1508.83804 1797.88360  19.2%   244 2751s\n",
      " 89920 10518     cutoff   38      1508.83804 1796.56045  19.1%   244 2756s\n",
      " 90350 10520 1759.13123   43   64 1508.83804 1795.21684  19.0%   244 2761s\n",
      " 90815 10561 1550.82292   43  144 1508.83804 1793.43546  18.9%   244 2765s\n",
      " 91708 10569     cutoff   47      1508.83804 1791.83268  18.8%   244 2774s\n",
      " 92198 10570     cutoff   44      1508.83804 1790.11182  18.6%   244 2806s\n",
      " 92249 10566     cutoff   44      1508.83804 1789.95617  18.6%   244 2810s\n",
      " 93083 10620     cutoff   44      1508.83804 1786.43839  18.4%   244 2819s\n",
      " 93624 10657     cutoff   45      1508.83804 1785.29167  18.3%   244 2823s\n",
      " 94139 10689 1653.79393   36  130 1508.83804 1784.17662  18.2%   244 2828s\n",
      " 94711 10713 1634.48919   41  132 1508.83804 1782.18919  18.1%   244 2832s\n",
      " 95062 10766 1589.96602   46  144 1508.83804 1781.30338  18.1%   244 2835s\n",
      " 95809 10766     cutoff   40      1508.83804 1779.94030  18.0%   244 2842s\n",
      " 96165 10785 1711.81216   43  163 1508.83804 1779.19142  17.9%   244 2846s\n",
      " 96603 10789 1616.66252   38  125 1508.83804 1778.12568  17.8%   244 2850s\n",
      " 97497 10823 1619.02889   37  158 1508.83804 1776.42671  17.7%   244 2858s\n",
      " 97957 10845     cutoff   39      1508.83804 1774.75225  17.6%   243 2861s\n",
      " 98335 10859 1586.02365   44   58 1508.83804 1774.75225  17.6%   243 2865s\n",
      " 99116 10855     cutoff   36      1508.83804 1772.19617  17.5%   243 2872s\n",
      " 99402 10856 1690.74420   39   85 1508.83804 1771.67934  17.4%   243 2877s\n",
      " 99728 10881 1611.36116   37  147 1508.83804 1771.67763  17.4%   244 2881s\n",
      " 100108 10885     cutoff   42      1508.83804 1771.67763  17.4%   243 2885s\n",
      " 100889 10885 1647.87533   40  287 1508.83804 1770.50806  17.3%   244 2893s\n",
      " 101244 10911     cutoff   41      1508.83804 1770.01715  17.3%   244 2897s\n",
      " 101764 10925 1537.65972   36  127 1508.83804 1768.91897  17.2%   243 2901s\n",
      " 102348 10896     cutoff   43      1508.83804 1768.23656  17.2%   243 2905s\n",
      " 103200 10871     cutoff   44      1508.83804 1765.53368  17.0%   244 2913s\n",
      " 103583 10876 1596.48542   39  130 1508.83804 1764.79864  17.0%   244 2916s\n",
      " 103867 10882     cutoff   46      1508.83804 1763.35135  16.9%   244 2920s\n",
      " 104869 10896     cutoff   44      1508.83804 1759.73182  16.6%   243 2929s\n",
      " 105407 10907     cutoff   37      1508.83804 1758.24026  16.5%   243 2933s\n",
      " 105974 10904 1522.93455   37    4 1508.83804 1756.05491  16.4%   243 2936s\n",
      " 106672 10929     cutoff   34      1508.83804 1753.87346  16.2%   243 2942s\n",
      " 107096 10920     cutoff   44      1508.83804 1752.50000  16.1%   243 2946s\n",
      " 107530 10912     cutoff   40      1508.83804 1749.73556  16.0%   243 2951s\n",
      " 108320 10942 1524.53426   42  143 1508.83804 1746.83148  15.8%   243 2958s\n",
      " 108785 10998 1741.70389   45  100 1508.83804 1745.49223  15.7%   243 2961s\n",
      " 109350 11024 1709.64275   39   64 1508.83804 1743.69810  15.6%   242 2965s\n",
      " 110346 11042 1575.19781   38  206 1508.83804 1739.99507  15.3%   242 2971s\n",
      " 111178 11058     cutoff   50      1508.83804 1737.65380  15.2%   242 2977s\n",
      " 111597 11048 1621.65452   41  144 1508.83804 1736.04257  15.1%   242 2980s\n",
      " 112447 11050 1551.12908   39  142 1508.83804 1732.13663  14.8%   242 2986s\n",
      " 113328 11075 1543.20808   47  158 1508.83804 1729.19107  14.6%   241 2992s\n",
      " 113830 11057 1530.10603   43   80 1508.83804 1726.94082  14.5%   241 2995s\n",
      " 114852 11043 1549.24433   35   83 1508.83804 1722.02020  14.1%   241 3001s\n",
      " 115783 11107 1531.75778   42    2 1508.83804 1718.68714  13.9%   241 3009s\n",
      " 116508 11123 1522.57210   40  100 1508.83804 1716.68966  13.8%   240 3012s\n",
      " 116942 11120 1577.51788   40  156 1508.83804 1715.58157  13.7%   240 3015s\n",
      " 117658 11206     cutoff   41      1508.83804 1713.50505  13.6%   240 3020s\n",
      " 118605 11209 1570.58101   38    4 1508.83804 1710.66759  13.4%   240 3026s\n",
      " 119505 11218     cutoff   38      1508.83804 1707.74638  13.2%   240 3031s\n",
      " 120415 11241 1546.83333   44    2 1508.83804 1703.55166  12.9%   239 3037s\n",
      " 121321 11271     cutoff   44      1508.83804 1700.55452  12.7%   239 3042s\n",
      " 121634 11291     cutoff   41      1508.83804 1700.09917  12.7%   239 3045s\n",
      " 122568 11393 1693.70305   43  128 1508.83804 1697.28813  12.5%   239 3050s\n",
      " 123451 11465     cutoff   47      1508.83804 1695.61497  12.4%   238 3055s\n",
      " 124469 11555     cutoff   40      1508.83804 1692.68091  12.2%   238 3061s\n",
      " 125477 11588 1571.43810   45    4 1508.83804 1690.75968  12.1%   237 3066s\n",
      " 126410 11620     cutoff   44      1508.83804 1688.50481  11.9%   237 3073s\n",
      " 127348 11646 1685.48485   40    4 1508.83804 1686.44886  11.8%   237 3075s\n",
      " 127877 11675     cutoff   43      1508.83804 1685.48485  11.7%   237 3080s\n",
      " 128696 11657 1539.12361   47  155 1508.83804 1683.73041  11.6%   236 3086s\n",
      " 129397 11651 1570.07509   47  144 1508.83804 1682.89398  11.5%   236 3091s\n",
      " 130318 11652     cutoff   45      1508.83804 1679.42742  11.3%   236 3096s\n",
      " 131360 11590     cutoff   38      1508.83804 1677.22568  11.2%   236 3101s\n",
      " 132285 11558 1542.04481   44    2 1508.83804 1673.99681  10.9%   235 3105s\n",
      " 133120 11528 1521.91892   37    2 1508.83804 1670.99267  10.7%   235 3110s\n",
      " 134022 11505 1576.08552   42  108 1508.83804 1668.53486  10.6%   235 3115s\n",
      " 135282 11506     cutoff   45      1508.83804 1665.21324  10.4%   234 3122s\n",
      " 135997 11464     cutoff   34      1508.83804 1663.30260  10.2%   234 3126s\n",
      " 136672 11421     cutoff   45      1508.83804 1662.03509  10.2%   234 3133s\n",
      " 137292 11427 1573.78208   35    4 1508.83804 1661.17262  10.1%   234 3136s\n",
      " 138363 11442 1527.01730   44   80 1508.83804 1658.91525  9.95%   234 3141s\n",
      " 139196 11396     cutoff   42      1508.83804 1658.44318  9.92%   233 3146s\n",
      " 140073 11377 1512.21721   44    2 1508.83804 1655.93023  9.75%   233 3151s\n",
      " 140874 11331 1648.93813   40  123 1508.83804 1652.90953  9.55%   233 3155s\n",
      " 141701 11322 1600.73158   38  150 1508.83804 1651.03158  9.42%   233 3160s\n",
      " 142383 11462 1626.62008   39  127 1508.83804 1650.37008  9.38%   233 3165s\n",
      " 142967 11415 1538.75513   39  154 1508.83804 1650.37008  9.38%   233 3170s\n",
      " 143950 11372     cutoff   44      1508.83804 1647.33945  9.18%   232 3176s\n",
      " 144892 11402     cutoff   44      1508.83804 1645.23158  9.04%   232 3181s\n",
      " 145522 11400 1568.04835   42  157 1508.83804 1644.53435  8.99%   232 3186s\n",
      " 146432 11429 1638.87777   41  146 1508.83804 1642.64202  8.87%   232 3191s\n",
      " 146957 11468 1624.04248   42  146 1508.83804 1641.98000  8.82%   232 3196s\n",
      " 148404 11526 1524.68261   44    2 1508.83804 1639.54545  8.66%   231 3200s\n",
      " 149359 11637     cutoff   40      1508.83804 1638.99941  8.63%   231 3206s\n",
      " 150085 11616 1613.55583   43  144 1508.83804 1638.10853  8.57%   231 3503s\n",
      " 150469 11601     cutoff   41      1508.83804 1637.59237  8.53%   231 3507s\n",
      " 150868 11614     cutoff   46      1508.83804 1637.04918  8.50%   230 3510s\n",
      " 151730 11608 1616.84078   36   64 1508.83804 1635.88684  8.42%   230 3517s\n",
      " 152256 11600 1628.44624   43  120 1508.83804 1635.09889  8.37%   230 3521s\n",
      " 153052 11606     cutoff   45      1508.83804 1633.69767  8.28%   230 3526s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 153889 11630 1559.43904   43    2 1508.83804 1632.37160  8.19%   230 3531s\n",
      " 154836 11649     cutoff   43      1508.83804 1630.87880  8.09%   229 3537s\n",
      " 155546 11683 1594.91805   35  165 1508.83804 1630.75676  8.08%   229 3541s\n",
      " 156176 11694 1526.43026   43   80 1508.83804 1630.07692  8.04%   229 3545s\n",
      " 156647 11691 1513.55028   44    2 1508.83804 1629.77778  8.02%   228 3550s\n",
      " 157479 11711     cutoff   51      1508.83804 1628.17054  7.91%   228 3555s\n",
      " 157900 11748     cutoff   44      1508.83804 1627.58498  7.87%   228 3560s\n",
      " 159175 11703 1577.82195   45  126 1508.83804 1626.04098  7.77%   228 3567s\n",
      " 159561 11695     cutoff   45      1508.83804 1626.01967  7.77%   228 3570s\n",
      " 160359 11776     cutoff   51      1508.83804 1624.88268  7.69%   227 3575s\n",
      " 160828 11778     cutoff   46      1508.83804 1624.88268  7.69%   227 3581s\n",
      " 161825 11817 1594.33333   44   83 1508.83804 1623.15985  7.58%   227 3586s\n",
      " 162789 11818 1518.57774   48    2 1508.83804 1622.35593  7.52%   227 3592s\n",
      " 163342 11832 1614.71547   42  120 1508.83804 1621.44288  7.46%   227 3595s\n",
      " 164322 11863 1583.58078   44  122 1508.83804 1620.12047  7.38%   226 3600s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 105\n",
      "  Implied bound: 3\n",
      "  MIR: 2593\n",
      "  Flow cover: 26\n",
      "  RLT: 2312\n",
      "  Relax-and-lift: 10\n",
      "  BQP: 232\n",
      "\n",
      "Explored 164535 nodes (37187782 simplex iterations) in 3599.30 seconds\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 10: 1508.84 1500.47 1490.31 ... 1411.25\n",
      "\n",
      "Time limit reached\n",
      "Best objective 1.508838043607e+03, best bound 1.620120467836e+03, gap 7.3754%\n",
      "[4, 3, 0.5, 5379, 68.97029131814249, 13.5736948370349, 611, 69.45, 13.750000000000002, 'branching = {1: 7, 2: 2, 3: 10}, treatments = {4: 1, 5: 4, 6: 5, 7: 4}']\n",
      "Changed value of parameter SolutionLimit to 1\n",
      "   Prev: 2000000000  Min: 1  Max: 2000000000  Default: 2000000000\n",
      "Gurobi Optimizer version 9.1.0 build v9.1.0rc0 (mac64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 22039 rows, 10025 columns and 73869 nonzeros\n",
      "Model fingerprint: 0x4f1ba668\n",
      "Variable types: 8002 continuous, 2023 integer (2023 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 6e+02]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 6e+02]\n",
      "Presolve removed 8013 rows and 4003 columns\n",
      "Presolve time: 0.29s\n",
      "Presolved: 14026 rows, 6022 columns, 50205 nonzeros\n",
      "Variable types: 4002 continuous, 2020 integer (2014 binary)\n",
      "\n",
      "Root relaxation: objective 2.000000e+03, 6764 iterations, 0.17 seconds\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0 2000.00000    0    6          - 2000.00000      -     -    0s\n",
      "H    0     0                    1328.8887086 2000.00000  50.5%     -    0s\n",
      "\n",
      "Explored 1 nodes (804 simplex iterations) in 0.98 seconds\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 1: 1328.89 \n",
      "\n",
      "Solution limit reached\n",
      "Best objective 1.328888708631e+03, best bound 2.000000000000e+03, gap 50.5017%\n",
      "Changed value of parameter TimeLimit to 3599.0148282051086\n",
      "   Prev: inf  Min: 0.0  Max: inf  Default: inf\n",
      "Changed value of parameter SolutionLimit to 1999999999\n",
      "   Prev: 1  Min: 1  Max: 2000000000  Default: 2000000000\n",
      "Gurobi Optimizer version 9.1.0 build v9.1.0rc0 (mac64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 22039 rows, 10025 columns and 73869 nonzeros\n",
      "Model fingerprint: 0x4f1ba668\n",
      "Variable types: 8002 continuous, 2023 integer (2023 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 6e+02]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 6e+02]\n",
      "Presolved: 14026 rows, 6022 columns, 50205 nonzeros\n",
      "\n",
      "Continuing optimization...\n",
      "\n",
      "     2     4 2000.00000    1 1467 1328.88871 2000.00000  50.5%   2.0   46s\n",
      "H   10    12                    1386.1743829 2000.00000  44.3%   344   47s\n",
      "    36    24 1787.30693    7    4 1386.17438 2000.00000  44.3%   510   50s\n",
      "   183    50 1712.06009   12    2 1386.17438 2000.00000  44.3%   554   55s\n",
      "   339    52     cutoff   11      1386.17438 1956.64336  41.2%   559   60s\n",
      "   469    37     cutoff    9      1386.17438 1883.62856  35.9%   576   65s\n",
      "\n",
      "Explored 581 nodes (340636 simplex iterations) in 67.30 seconds\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 2: 1386.17 1328.89 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 1.386174382941e+03, best bound 1.386174382941e+03, gap 0.0000%\n",
      "[5, 2, 0.5, 4907, 71.69310643207383, 11.15661955581194, 566, 71.7, 10.4, 'branching = {1: 10}, treatments = {2: 1, 3: 4}']\n",
      "Changed value of parameter SolutionLimit to 1\n",
      "   Prev: 2000000000  Min: 1  Max: 2000000000  Default: 2000000000\n",
      "Gurobi Optimizer version 9.1.0 build v9.1.0rc0 (mac64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 54079 rows, 22061 columns and 184643 nonzeros\n",
      "Model fingerprint: 0xb3d446ea\n",
      "Variable types: 16004 continuous, 6057 integer (6057 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 6e+02]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 6e+02]\n",
      "Presolve removed 45 rows and 27 columns\n",
      "Presolve time: 0.53s\n",
      "Presolved: 54034 rows, 22034 columns, 176596 nonzeros\n",
      "Variable types: 16004 continuous, 6030 integer (6030 binary)\n",
      "Found heuristic solution: objective 1250.0000000\n",
      "\n",
      "Explored 0 nodes (0 simplex iterations) in 0.76 seconds\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 1: 1250 \n",
      "\n",
      "Solution limit reached\n",
      "Best objective 1.250000000000e+03, best bound 8.000000000000e+03, gap 540.0000%\n",
      "Changed value of parameter TimeLimit to 3599.232336997986\n",
      "   Prev: inf  Min: 0.0  Max: inf  Default: inf\n",
      "Changed value of parameter SolutionLimit to 1999999999\n",
      "   Prev: 1  Min: 1  Max: 2000000000  Default: 2000000000\n",
      "Gurobi Optimizer version 9.1.0 build v9.1.0rc0 (mac64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 54079 rows, 22061 columns and 184643 nonzeros\n",
      "Model fingerprint: 0xb3d446ea\n",
      "Variable types: 16004 continuous, 6057 integer (6057 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 6e+02]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 6e+02]\n",
      "Presolved: 54034 rows, 22034 columns, 176596 nonzeros\n",
      "\n",
      "Continuing optimization...\n",
      "\n",
      "\n",
      "Root relaxation: objective 3.976000e+03, 27821 iterations, 1.66 seconds\n"
     ]
    }
   ],
   "source": [
    "kallus = pd.DataFrame(columns=['Dataset','Depth','P(Correct Treatment)','Test Error','Test % Optimal',\n",
    "                               'Test % Same Treatment', 'Train Error', 'Train % Optimal', \n",
    "                               'Train % Same Treatment', 'Tree'])\n",
    "\n",
    "\n",
    "prob = 0.5\n",
    "bert = False\n",
    "\n",
    "depths = [2, 3]\n",
    "datasets = [3, 4, 5]\n",
    "#probs = [0.1, 0.5, 0.9]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for d in depths:\n",
    "        train_filepath = 'data/IST_2000_binary/data_train_enc_' + str(dataset) + '.csv'\n",
    "        test_filepath = 'data/IST_2000_binary/data_test_enc_' + str(dataset) + '.csv'\n",
    "        train_X, train_t, train_y, train_real = open_file(train_filepath)\n",
    "        branching, treatments = run_tree(d, bert)\n",
    "\n",
    "        tree = Tree(d)\n",
    "        L = set(range(2**(d-1), 2**d))\n",
    "        test_X, test_t, test_y, test_real = open_file(test_filepath)\n",
    "        error, pct, acc = get_metrics(test_X, test_real, test_t)\n",
    "        error1, pct1, acc1 = get_metrics(train_X, train_real, train_t)\n",
    "\n",
    "        tree_stats = 'branching = ' + str(branching) + ', treatments = ' + str(treatments)\n",
    "        row = [dataset, d, prob, error, pct*100, acc*100, error1, pct1*100, acc1*100, tree_stats]\n",
    "        print(row)\n",
    "        if bert:\n",
    "            bertsimas.loc[len(bertsimas)] = row\n",
    "        else:\n",
    "            kallus.loc[len(kallus)] = row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kallus.to_csv('Results_IST_binary/kallus2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed value of parameter TimeLimit to 3600.0\n",
      "   Prev: inf  Min: 0.0  Max: inf  Default: inf\n",
      "Gurobi Optimizer version 9.1.0 build v9.1.0rc0 (mac64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 22039 rows, 10039 columns and 99696 nonzeros\n",
      "Model fingerprint: 0xb0b31bf7\n",
      "Variable types: 8002 continuous, 2037 integer (2037 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [3e-02, 6e+02]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [9e-01, 6e+02]\n",
      "Presolve removed 8034 rows and 4022 columns\n",
      "Presolve time: 0.58s\n",
      "Presolved: 14005 rows, 6017 columns, 79167 nonzeros\n",
      "Variable types: 4002 continuous, 2015 integer (2015 binary)\n",
      "\n",
      "Root relaxation: objective 1.897781e+03, 7002 iterations, 0.98 seconds\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0 1897.78083    0  568          - 1897.78083      -     -    2s\n",
      "H    0     0                     914.8065581 1897.78083   107%     -    2s\n",
      "H    0     0                     954.4295127 1897.78083  98.8%     -    3s\n",
      "     0     0 1897.78083    0 1681  954.42951 1897.78083  98.8%     -    8s\n",
      "H    0     0                     967.9462660 1897.78083  96.1%     -    8s\n",
      "     0     0 1897.78083    0 1473  967.94627 1897.78083  96.1%     -    8s\n",
      "     0     0 1897.78083    0 1529  967.94627 1897.78083  96.1%     -   11s\n",
      "     0     0 1897.78083    0 1529  967.94627 1897.78083  96.1%     -   11s\n",
      "     0     0 1897.78083    0  798  967.94627 1897.78083  96.1%     -   12s\n",
      "     0     0 1897.78083    0  796  967.94627 1897.78083  96.1%     -   12s\n",
      "H    0     0                    1020.6339109 1897.78083  85.9%     -   12s\n",
      "     0     0 1897.78083    0 1200 1020.63391 1897.78083  85.9%     -   13s\n",
      "     0     0 1897.78083    0 1200 1020.63391 1897.78083  85.9%     -   13s\n",
      "     0     0 1897.78083    0 1489 1020.63391 1897.78083  85.9%     -   14s\n",
      "     0     0 1897.78083    0 1489 1020.63391 1897.78083  85.9%     -   14s\n",
      "     0     2 1897.78083    0 1489 1020.63391 1897.78083  85.9%     -   15s\n",
      "    58    45     cutoff   12      1020.63391 1897.78083  85.9%   457   20s\n",
      "*   83    55              16    1027.4409749 1897.78083  84.7%   396   22s\n",
      "   137    67 1367.36946    9    2 1027.44097 1897.78083  84.7%   503   25s\n",
      "   219    91 1354.92210   12    2 1027.44097 1897.78083  84.7%   500   30s\n",
      "   279   115 1280.23208   11  692 1027.44097 1897.78083  84.7%   497   35s\n",
      "   372   139 1385.56397   22    2 1027.44097 1887.30698  83.7%   476   40s\n",
      "   488   137     cutoff   15      1027.44097 1837.14963  78.8%   512   45s\n",
      "   559   137 1653.62072   16    4 1027.44097 1808.69517  76.0%   524   50s\n",
      "   713   140 1318.62699    5    2 1027.44097 1759.19970  71.2%   540   55s\n",
      "   938   118     cutoff   13      1027.44097 1528.12742  48.7%   517   61s\n",
      "  1215    60     cutoff   20      1027.44097 1248.92886  21.6%   496   65s\n",
      "\n",
      "Cutting planes:\n",
      "  MIR: 16\n",
      "  Flow cover: 3\n",
      "  RLT: 11\n",
      "\n",
      "Explored 1390 nodes (695145 simplex iterations) in 67.25 seconds\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 5: 1027.44 1020.63 967.946 ... 914.807\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 1.027440974920e+03, best bound 1.027440974920e+03, gap 0.0000%\n",
      "[4, 2, 1997.4432696442377, 24.39164701144797, 11.338664212161307, 231.808278142224, 23.75, 11.600000000000001, 'branching = {1: 23}, treatments = {2: 4, 3: 1}']\n"
     ]
    }
   ],
   "source": [
    "dataset = 4\n",
    "d = 2\n",
    "bert = False\n",
    "train_filepath = 'data/IST_2000/data_train_enc_' + str(dataset) + '.csv'\n",
    "test_filepath = 'data/IST_2000/data_test_enc_' + str(dataset) + '.csv'\n",
    "train_X, train_t, train_y, train_real = open_file(train_filepath)\n",
    "branching, treatments = run_tree(d, bert)\n",
    "\n",
    "tree = Tree(d)\n",
    "L = set(range(2**(d-1), 2**d))\n",
    "test_X, test_t, test_y, test_real = open_file(test_filepath)\n",
    "error, pct, acc = get_metrics(test_X, test_real, test_t)\n",
    "error1, pct1, acc1 = get_metrics(train_X, train_real, train_t)\n",
    "\n",
    "tree_stats = 'branching = ' + str(branching) + ', treatments = ' + str(treatments)\n",
    "row = [dataset, d, error, pct*100, acc*100, error1, pct1*100, acc1*100, tree_stats]\n",
    "print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267.9863312540136 0.4446 0.5066\n",
      "21.252969854711775 0.354 0.5\n"
     ]
    }
   ],
   "source": [
    "tree = Tree(d)\n",
    "L = set(range(2**(d-1), 2**d))\n",
    "test_X, test_t, test_y, test_real = open_file(test_filepath)\n",
    "error, pct, acc = get_metrics(test_X, test_real, test_t)\n",
    "error1, pct1, acc1 = get_metrics(train_X, train_real, train_t)\n",
    "print(error, pct, acc)\n",
    "print(error1, pct1, acc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 3, 0.9, 267.9863312540136, 44.46, 50.660000000000004, 21.252969854711775, 35.4, 50.0, 'branching = {1: 10, 2: 16, 3: 0}, treatments = {4: 1, 5: 1, 6: 1, 7: 1}']\n"
     ]
    }
   ],
   "source": [
    "tree_stats = 'branching = ' + str(branching) + ', treatments = ' + str(treatments)\n",
    "row = [dataset, d, prob, error, pct*100, acc*100, error1, pct1*100, acc1*100, tree_stats]\n",
    "if bert:\n",
    "    bertsimas.loc[len(bertsimas)] = row\n",
    "else:\n",
    "    kallus.loc[len(kallus)] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Dataset  Depth  P(Correct Treatment)  Test Error  Test % Optimal  \\\n",
      "0         1      2                   0.5   53.227602           76.83   \n",
      "1         1      3                   0.5  123.833930           66.00   \n",
      "2         1      3                   0.5  123.833930           66.00   \n",
      "3         2      2                   0.5   24.915258           84.31   \n",
      "4         2      3                   0.5   24.915258           84.31   \n",
      "5         3      2                   0.5  166.280038           58.61   \n",
      "6         3      3                   0.5   57.544135           77.50   \n",
      "7         4      2                   0.5  432.810829           24.05   \n",
      "8         4      3                   0.5  231.607156           44.69   \n",
      "9         5      2                   0.5  189.645268           55.55   \n",
      "10        5      3                   0.5   36.072978           82.59   \n",
      "11        1      2                   0.9  224.473595           49.78   \n",
      "12        1      3                   0.9  224.473595           49.78   \n",
      "13        2      2                   0.9  216.993310           50.34   \n",
      "14        2      3                   0.9  216.993310           50.34   \n",
      "15        3      2                   0.9  279.888354           41.39   \n",
      "16        3      3                   0.9  290.892989           40.41   \n",
      "17        4      2                   0.9   75.234129           75.97   \n",
      "18        4      3                   0.9   75.234129           75.97   \n",
      "19        5      2                   0.9  267.986331           44.46   \n",
      "20        5      3                   0.9  267.986331           44.46   \n",
      "\n",
      "    Test % Same Treatment  Train Error  Train % Optimal  \\\n",
      "0                   49.33     1.833834             83.0   \n",
      "1                   49.34     2.356756             85.2   \n",
      "2                   49.34     2.356756             85.2   \n",
      "3                   50.46     1.220292             87.8   \n",
      "4                   50.46     1.220292             87.8   \n",
      "5                   49.19     5.115352             75.0   \n",
      "6                   49.48     2.759458             81.4   \n",
      "7                   50.07     2.819187             83.8   \n",
      "8                   50.66     0.674055             91.8   \n",
      "9                   49.63     8.600535             64.8   \n",
      "10                  48.97     0.969443             87.0   \n",
      "11                  50.18     5.508442             73.6   \n",
      "12                  50.18     5.508442             73.6   \n",
      "13                  50.72    15.210286             48.6   \n",
      "14                  50.72    15.210286             48.6   \n",
      "15                  50.68    31.611031             25.0   \n",
      "16                  49.90    31.992727             24.2   \n",
      "17                  49.97    32.501338             16.6   \n",
      "18                  49.97    32.501338             16.6   \n",
      "19                  50.66    21.252970             35.4   \n",
      "20                  50.66    21.252970             35.4   \n",
      "\n",
      "    Train % Same Treatment                                               Tree  \n",
      "0                     51.4      branching = {1: 2}, treatments = {2: 1, 3: 0}  \n",
      "1                     51.4  branching = {1: 16, 2: 5, 3: 12}, treatments =...  \n",
      "2                     51.4  branching = {1: 16, 2: 5, 3: 12}, treatments =...  \n",
      "3                     49.6      branching = {1: 3}, treatments = {2: 1, 3: 0}  \n",
      "4                     49.6  branching = {1: 3, 2: 13, 3: 15}, treatments =...  \n",
      "5                     48.6     branching = {1: 12}, treatments = {2: 0, 3: 0}  \n",
      "6                     46.6  branching = {1: 5, 2: 14, 3: 12}, treatments =...  \n",
      "7                     52.6     branching = {1: 13}, treatments = {2: 0, 3: 0}  \n",
      "8                     48.0  branching = {1: 7, 2: 10, 3: 13}, treatments =...  \n",
      "9                     51.0     branching = {1: 15}, treatments = {2: 0, 3: 0}  \n",
      "10                    49.2  branching = {1: 15, 2: 7, 3: 5}, treatments = ...  \n",
      "11                    48.4     branching = {1: 15}, treatments = {2: 1, 3: 1}  \n",
      "12                    48.4  branching = {1: 10, 2: 15, 3: 0}, treatments =...  \n",
      "13                    51.2     branching = {1: 14}, treatments = {2: 1, 3: 1}  \n",
      "14                    51.2  branching = {1: 16, 2: 0, 3: 11}, treatments =...  \n",
      "15                    49.4     branching = {1: 15}, treatments = {2: 1, 3: 1}  \n",
      "16                    48.6  branching = {1: 10, 2: 16, 3: 8}, treatments =...  \n",
      "17                    50.0     branching = {1: 14}, treatments = {2: 1, 3: 1}  \n",
      "18                    50.0  branching = {1: 16, 2: 0, 3: 14}, treatments =...  \n",
      "19                    50.0     branching = {1: 16}, treatments = {2: 1, 3: 1}  \n",
      "20                    50.0  branching = {1: 10, 2: 16, 3: 0}, treatments =...  \n"
     ]
    }
   ],
   "source": [
    "print(bertsimas)\n",
    "bertsimas.to_csv('bertsimas_athey_500.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Performance for Athey's Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMMARY: KALLUS on Athey\n",
    "summary = {'Method': ['Dataset 1', 'Dataset 1', 'Dataset 2', 'Dataset 2', 'Dataset 3',\n",
    "                     'Dataset 3', 'Dataset 4', 'Dataset 4', 'Dataset 5', 'Dataset 5', 'Dataset 1', \n",
    "                      'Dataset 1', 'Dataset 2', 'Dataset 2', 'Dataset 3',\n",
    "                     'Dataset 3', 'Dataset 4', 'Dataset 4', 'Dataset 5', 'Dataset 5'],\n",
    "           'Depth': [2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3],\n",
    "           'P(Correct Treatment)': ['0.5', '0.5', '0.5', '0.5', '0.5', '0.5', '0.5', '0.5', '0.5', '0.5', '0.9', '0.9',\n",
    "                                   '0.9', '0.9', '0.9', '0.9', '0.9', '0.9', '0.9', '0.9'],\n",
    "          'Error': [74.67, 74.67, 46.40, 79.86, 145, 109.21, 18.15, 18.15, 152.55, 90.47, 199.70, 238.67, 219.02, \n",
    "                    239.10, 201.21, 201.21, 99.23, 88.12, 196.07, 156.84],\n",
    "          '% Classified': [71.97, 71.97, 78.49, 71.11, 61.22, 68.03, 88.33, 88.33, 60.41, 70.03, 53.35,\n",
    "                          47.49, 49.91, 47.49, 53.42, 53.42, 71.19, 74.11, 54.12, 58.02]}\n",
    "summary = pd.DataFrame(summary)\n",
    "summary.to_csv('kallus_tree_athey.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMMARY: Bertsimas on Athey\n",
    "summary = {'Method': ['Dataset 1', 'Dataset 1', 'Dataset 2', 'Dataset 2', 'Dataset 3',\n",
    "                     'Dataset 3', 'Dataset 4', 'Dataset 4', 'Dataset 5', 'Dataset 5', 'Dataset 1', \n",
    "                      'Dataset 1', 'Dataset 2', 'Dataset 2', 'Dataset 3',\n",
    "                     'Dataset 3', 'Dataset 4', 'Dataset 4', 'Dataset 5', 'Dataset 5'],\n",
    "           'Depth': [2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3],\n",
    "           'P(Correct Treatment)': ['0.5', '0.5', '0.5', '0.5', '0.5', '0.5', '0.5', '0.5', '0.5', '0.5', '0.9', '0.9',\n",
    "                                   '0.9', '0.9', '0.9', '0.9', '0.9', '0.9', '0.9', '0.9'],\n",
    "          'Error': [74.67, 74.67, 46.40, 55.17, 145, 109.21, 18.15, 18.15, 152.55, 90.47, 199.70, 154.07, 219.02, \n",
    "                    180.83, 201.21, 116.75, 99.23, 88.12, 196.07, 156.84],\n",
    "          '% Classified': [71.97, 71.97, 78.49, 76.49, 61.22, 68.03, 88.33, 88.33, 60.41, 70.03, 53.35,\n",
    "                          62.17, 49.91, 53.91, 53.42, 71.59, 71.19, 74.11, 54.12, 58.02]}\n",
    "summary = pd.DataFrame(summary)\n",
    "summary.to_csv('bertsimas_tree_athey.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datapoint_tree_avg(node, i):\n",
    "    if node in L: #if datapoint has reached leaf node, calculate error\n",
    "        return node\n",
    "    if train_X.iloc[i, branching[node]] <= 0: # go left (node 2)\n",
    "        return datapoint_tree_avg(tree.get_left_children(node), i)\n",
    "    else:\n",
    "        return datapoint_tree_avg(tree.get_right_children(node), i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summation = {20: 0, 21: 0, 30: 0, 31: 0}\n",
    "count = {20: 0, 21: 0, 30: 0, 31: 0}\n",
    "\n",
    "for i in range(n):\n",
    "    leaf_node = datapoint_tree_avg(1, i)\n",
    "    index = str(leaf_node) + str(train_t[i])\n",
    "    summation[int(index)] += train_y[i]\n",
    "    count[int(index)] += 1\n",
    "    \n",
    "avg = {}\n",
    "for i in summation:\n",
    "    avg[i] = float(summation[i]) / count[i]\n",
    "\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 0.39851651102745433, 3: 0.34277517675515534}\n"
     ]
    }
   ],
   "source": [
    "summation = {2: 0, 3: 0}\n",
    "count = {2: 0, 3: 0}\n",
    "\n",
    "for i in range(n):\n",
    "    leaf_node = datapoint_tree_avg(1, i)\n",
    "    if train_t[i] == treatments[leaf_node]:\n",
    "        summation[leaf_node] += train_y[i]\n",
    "        count[leaf_node] += 1\n",
    "    \n",
    "avg = {}\n",
    "for i in summation:\n",
    "    avg[i] = float(summation[i]) / count[i]\n",
    "\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2, 3}\n"
     ]
    }
   ],
   "source": [
    "N = L.union(L_c)\n",
    "print(N)\n",
    "\n",
    "model = gp.Model(\"Nathan\")\n",
    "\n",
    "# -- VARIABLE DECLARATION --\n",
    "\n",
    "# -- Variables to determine: gamma and lambda --\n",
    "# 1. gamma_p = choice of cut at node p ([0, 1]^C_p) (only applies to non-leaf node)\n",
    "#       - represent with a matrix gamma (|L_c| x |C_p|)\n",
    "\n",
    "gamma = model.addVars(L_c, len(C), vtype=GRB.BINARY, name='gamma') \n",
    "# This assumes gamma is binary\n",
    "#gamma = model.addVars(L_c, len(C), vtype=GRB.BINARY, name='gamma') \n",
    "\n",
    "# 2. lambda_pt = choice of treatment t at node p (only applies to leaf nodes L)\n",
    "#       - represent with a matrix lamb (|L| x m)\n",
    "lamb = model.addVars(L, m, vtype=GRB.BINARY, name='lamb')\n",
    "\n",
    "\n",
    "# -- Other Variables in Formulation --\n",
    "# 1. w_ip = membership of datapoint i in node p (only applies to leaf nodes L)\n",
    "#       - represent with a matrix w (n x |L|)\n",
    "\n",
    "c = model.addVars(n, N, vtype=GRB.BINARY, name='c')\n",
    "# This assumes w is binary, when in reality it is continuous from 0-1\n",
    "#w = model.addVars(n, L, vtype=GRB.BINARY, name='w') # Original paper has this be a continuous variable\n",
    "\n",
    "# 2. mu_p = mean outcome of prescribed treatment in node p\n",
    "#       - represent with a matrix mu (|L|)\n",
    "v = model.addVars(n, L_c, vtype=GRB.BINARY, name='v') # define in constraint\n",
    "\n",
    "# 3. nu_ip = \"effect\" of treatment in node p by multiplying mu and w\n",
    "#       - represent with a matrix nu (n x |L|)\n",
    "w = model.addVars(n, vtype=GRB.BINARY, name='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"if different_Cp:\n",
    "# ---- K and Z if we followed the definition of C_p from the paper -----\n",
    "    for p in L_c:\n",
    "        k[p] = math.ceil(math.log2(len(C[p])))\n",
    "\n",
    "    z = {}\n",
    "    for p in L_c:\n",
    "        matrix = np.zeros((k[p], len(C[p])))\n",
    "        print(matrix.shape)\n",
    "        for i in range(1, k[p]+1):\n",
    "            for j in range(1, len(C[p])+1):\n",
    "                if math.floor(j/(2**i)) % 2 == 1: # odd number\n",
    "                    z[i-1, j-1] = 1\n",
    "                else:\n",
    "                    z[i-1, j-1] = 0\n",
    "        z[p] = matrix\n",
    "\n",
    "else:\n",
    "    # ---- K and Z if we had constant C for all nodes ----\n",
    "    k = math.ceil(math.log2(len(C)))\n",
    "    z = np.zeros((k, len(C)))\n",
    "    for i in range(1, k+1):\n",
    "        for j in range(1, len(C)+1):\n",
    "            if math.floor(j/(2**i)) % 2 == 1: # odd number\n",
    "                z[i-1, j-1] = 1\n",
    "            else:\n",
    "                z[i-1, j-1] = 0\n",
    "print(z)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
